import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import argparse
from pathlib import Path
import sys
import os
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from fusion_model import ExactCrossAttentionFusion, create_exact_fusion_model
from data_loader import ExactDataLoader, create_exact_data_loader


sys.path.append('PP-cVAE')

try:

    original_path = sys.path.copy()
    sys.path.insert(0, 'PP-cVAE')
    
    from config import Config as VAEConfig
    

    sys.path = original_path
    
except ImportError:
    print("ä½¿ç”¨é»˜è®¤é…ç½®")
    class VAEConfig:
        LATENT_DIM = 128
        NUM_PLANES = 9
        DROPOUT_RATE = 0.3

class ExactFusionDataset(Dataset):
    """
    åŸºäºåŸå§‹æ•°æ®æ ¼å¼çš„èåˆæ•°æ®é›†
    """
    
    def __init__(self, data_dict):
        self.vae_data = data_dict['vae_data']
        self.descriptors = data_dict['descriptors']
        self.graph_cc = data_dict['graph_cc']
        self.graph_noncc = data_dict['graph_noncc']
        self.lzhnn_data = data_dict['lzhnn_data']
        self.cc_mask = data_dict['cc_mask']
        self.targets = data_dict['targets']
        self.identifiers = data_dict['identifiers']
        
    def __len__(self):
        return len(self.vae_data)
    
    def __getitem__(self, idx):
        sample = {
            'vae_data': self.vae_data[idx],
            'graph_cc': self.graph_cc[idx] if self.graph_cc else None,
            'graph_noncc': self.graph_noncc[idx] if self.graph_noncc else None,
            'lzhnn_data': {
                'topo': self.lzhnn_data['topo'][idx],
                'struct': self.lzhnn_data['struct'][idx]
            },
            'cc_mask': self.cc_mask[idx],
            'targets': self.targets[idx],
            'identifier': self.identifiers[idx]
        }
        
        if self.descriptors is not None:
            sample['descriptors'] = self.descriptors[idx]
            
        return sample

def exact_collate_fn(batch):
    """
    åŸºäºåŸå§‹æ•°æ®æ ¼å¼çš„collateå‡½æ•°
    """
    # å¤„ç†tensoræ•°æ®
    vae_data = torch.stack([item['vae_data'] for item in batch])
    cc_mask = torch.stack([item['cc_mask'] for item in batch])
    targets = torch.stack([item['targets'] for item in batch])
    
    # å¤„ç†lzhnnæ•°æ®
    lzhnn_data = {
        'topo': torch.stack([item['lzhnn_data']['topo'] for item in batch]),
        'struct': torch.stack([item['lzhnn_data']['struct'] for item in batch])
    }
    
    # å¤„ç†å›¾æ•°æ®
    graph_cc = [item['graph_cc'] for item in batch]
    graph_noncc = [item['graph_noncc'] for item in batch]
    
    # å¤„ç†æè¿°ç¬¦
    descriptors = None
    if batch[0].get('descriptors') is not None:
        descriptors = torch.stack([item['descriptors'] for item in batch])
    
    # å¤„ç†æ ‡è¯†ç¬¦
    identifiers = [item['identifier'] for item in batch]
    
    return {
        'vae_data': vae_data,
        'descriptors': descriptors,
        'graph_cc': graph_cc,
        'graph_noncc': graph_noncc,
        'lzhnn_data': lzhnn_data,
        'cc_mask': cc_mask,
        'targets': targets,
        'identifiers': identifiers
    }

class ExactFusionLoss(nn.Module):
    """
    åŸºäºåŸå§‹æŸå¤±å‡½æ•°çš„èåˆæŸå¤±
    """
    
    def __init__(self, main_weight=0.8, fusion_weight=0.2):
        super(ExactFusionLoss, self).__init__()
        self.main_weight = main_weight
        self.fusion_weight = fusion_weight
        self.mse_loss = nn.MSELoss()
        
    def forward(self, outputs, targets):
        # ä¸»è¦æŸå¤±ï¼šèåˆé¢„æµ‹ vs ç›®æ ‡
        fusion_loss = self.mse_loss(outputs['prediction'], targets)
        
        # VAEæŸå¤±ï¼šVAEé¢„æµ‹ vs ç›®æ ‡
        vae_loss = self.mse_loss(outputs['vae_prediction'], targets)
        
        # æ€»æŸå¤±
        total_loss = self.main_weight * fusion_loss + (1 - self.main_weight) * vae_loss
        
        return {
            'total_loss': total_loss,
            'fusion_loss': fusion_loss,
            'vae_loss': vae_loss
        }

class ExactEarlyStopping:
    """
    åŸºäºåŸå§‹early stoppingçš„å®ç°
    """
    
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.counter = 0
        
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False  # ä¸åœæ­¢
        else:
            self.counter += 1
            return self.counter >= self.patience  # æ˜¯å¦åœæ­¢

def exact_train_epoch(model, dataloader, criterion, optimizer, device):
    """
    åŸºäºåŸå§‹è®­ç»ƒé€»è¾‘çš„è®­ç»ƒepoch
    """
    model.train()
    total_loss = 0.0
    num_batches = len(dataloader)
    
    for batch_idx, batch in enumerate(dataloader):
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        for key in ['vae_data', 'cc_mask', 'targets']:
            if key in batch and batch[key] is not None:
                batch[key] = batch[key].to(device)
        
        if batch['descriptors'] is not None:
            batch['descriptors'] = batch['descriptors'].to(device)
            
        # ä¿®å¤ï¼šåªç§»åŠ¨å®é™…å­˜åœ¨çš„lzhnnæ•°æ®é”®
        for key in ['topo', 'struct']:
            if key in batch['lzhnn_data'] and batch['lzhnn_data'][key] is not None:
                batch['lzhnn_data'][key] = batch['lzhnn_data'][key].to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(batch)
        
        # è®¡ç®—æŸå¤±
        loss_dict = criterion(outputs, batch['targets'])
        loss = loss_dict['total_loss']
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if batch_idx % 10 == 0:
            print(f"  Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}")
    
    return total_loss / num_batches

def exact_validate_epoch(model, dataloader, criterion, device):
    """
    åŸºäºåŸå§‹éªŒè¯é€»è¾‘çš„éªŒè¯epoch
    """
    model.eval()
    total_loss = 0.0
    num_batches = len(dataloader)
    
    with torch.no_grad():
        for batch in dataloader:
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            for key in ['vae_data', 'cc_mask', 'targets']:
                if key in batch and batch[key] is not None:
                    batch[key] = batch[key].to(device)
            
            if batch['descriptors'] is not None:
                batch['descriptors'] = batch['descriptors'].to(device)
                
            for key in ['topo', 'struct']:
                batch['lzhnn_data'][key] = batch['lzhnn_data'][key].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch)
            
            # è®¡ç®—æŸå¤±
            loss_dict = criterion(outputs, batch['targets'])
            loss = loss_dict['total_loss']
            
            total_loss += loss.item()
    
    return total_loss / num_batches

def calculate_regression_metrics(model, dataloader, criterion, device, label_scaler=None):
    """
    è®¡ç®—å›å½’æŒ‡æ ‡ï¼ˆåŸå€¼ç©ºé—´ï¼‰ï¼šRÂ²ã€RMSEã€MAEã€çš®å°”æ£®ã€æ–¯çš®å°”æ›¼
    """
    model.eval()
    all_predictions = []
    all_targets = []
    total_loss = 0.0
    num_batches = len(dataloader)
    
    with torch.no_grad():
        for batch in dataloader:
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            for key in ['vae_data', 'cc_mask', 'targets']:
                if key in batch and batch[key] is not None:
                    batch[key] = batch[key].to(device)
            
            if batch['descriptors'] is not None:
                batch['descriptors'] = batch['descriptors'].to(device)
                
            for key in ['topo', 'struct']:
                if key in batch['lzhnn_data'] and batch['lzhnn_data'][key] is not None:
                    batch['lzhnn_data'][key] = batch['lzhnn_data'][key].to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch)
            
            # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡ï¼ˆæ ‡å‡†åŒ–ç©ºé—´ï¼‰
            predictions = outputs['prediction'].cpu().numpy().flatten()
            targets = batch['targets'].cpu().numpy().flatten()
            
            all_predictions.extend(predictions)
            all_targets.extend(targets)
            
            # è®¡ç®—æŸå¤±
            loss_dict = criterion(outputs, batch['targets'])
            total_loss += loss_dict['total_loss'].item()
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)
    
    # åæ ‡å‡†åŒ–åˆ°åŸå€¼ç©ºé—´
    if label_scaler is not None:
        # ä½¿ç”¨StandardScalerè¿›è¡Œåæ ‡å‡†åŒ–
        predictions_original = label_scaler.inverse_transform(all_predictions.reshape(-1, 1)).flatten()
        targets_original = label_scaler.inverse_transform(all_targets.reshape(-1, 1)).flatten()
        
        print(f"  æ ‡å‡†åŒ–ç©ºé—´ - é¢„æµ‹èŒƒå›´: [{all_predictions.min():.4f}, {all_predictions.max():.4f}]")
        print(f"  æ ‡å‡†åŒ–ç©ºé—´ - ç›®æ ‡èŒƒå›´: [{all_targets.min():.4f}, {all_targets.max():.4f}]")
        print(f"  åŸå€¼ç©ºé—´ - é¢„æµ‹èŒƒå›´: [{predictions_original.min():.4f}, {predictions_original.max():.4f}]")
        print(f"  åŸå€¼ç©ºé—´ - ç›®æ ‡èŒƒå›´: [{targets_original.min():.4f}, {targets_original.max():.4f}]")
    else:
        # å¦‚æœæ²¡æœ‰æ ‡å‡†åŒ–å™¨ï¼Œç›´æ¥ä½¿ç”¨åŸå€¼
        predictions_original = all_predictions
        targets_original = all_targets
        print("  æ³¨æ„: æœªæä¾›æ ‡ç­¾æ ‡å‡†åŒ–å™¨ï¼Œåœ¨æ ‡å‡†åŒ–ç©ºé—´è®¡ç®—æŒ‡æ ‡")
    
    # åœ¨åŸå€¼ç©ºé—´è®¡ç®—å›å½’æŒ‡æ ‡
    mse = mean_squared_error(targets_original, predictions_original)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(targets_original, predictions_original)
    r2 = r2_score(targets_original, predictions_original)
    # ç›¸å…³æ€§æŒ‡æ ‡
    try:
        from scipy.stats import pearsonr, spearmanr
        pearson_r, _ = pearsonr(targets_original, predictions_original)
        spearman_rho, _ = spearmanr(targets_original, predictions_original)
    except Exception as e:
        print(f"  è®¡ç®—ç›¸å…³æ€§å¤±è´¥: {e}")
        pearson_r, spearman_rho = float('nan'), float('nan')
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / num_batches
    
    return {
        'loss': avg_loss,
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'pearson': pearson_r,
        'spearman': spearman_rho,
        'predictions': predictions_original,
        'targets': targets_original
    }

def train_single_fold(model, train_data, val_data, test_data, args, device, fold_idx, output_dir, label_scaler=None):
    """
    è®­ç»ƒå•ä¸ªæŠ˜çš„æ¨¡å‹
    """
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒç¬¬{fold_idx + 1}æŠ˜")
    print(f"{'='*60}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = ExactFusionDataset(train_data)
    val_dataset = ExactFusionDataset(val_data)
    test_dataset = ExactFusionDataset(test_data)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        collate_fn=exact_collate_fn
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        collate_fn=exact_collate_fn
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        collate_fn=exact_collate_fn
    )
    
    # è®­ç»ƒé…ç½®
    criterion = ExactFusionLoss()
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=8, factor=0.7, min_lr=1e-6
    )
    early_stopping = ExactEarlyStopping(patience=args.early_stopping_patience)
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    for epoch in range(args.num_epochs):
        # è®­ç»ƒ
        train_loss = exact_train_epoch(model, train_loader, criterion, optimizer, device)
        train_losses.append(train_loss)
        
        # éªŒè¯
        val_loss = exact_validate_epoch(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_save_path = output_dir / f'best_model_fold_{fold_idx + 1}.pth'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'train_loss': train_loss,
                'fold': fold_idx + 1
            }, model_save_path)
        
        # Early stopping
        if early_stopping(val_loss):
            print(f"ç¬¬{fold_idx + 1}æŠ˜ Early stopping at epoch {epoch+1}")
            break
    
    # æµ‹è¯•è¯„ä¼° - è®¡ç®—è¯¦ç»†çš„å›å½’æŒ‡æ ‡ï¼ˆåœ¨åŸå€¼ç©ºé—´ï¼‰
    test_metrics = calculate_regression_metrics(model, test_loader, criterion, device, label_scaler)
    
    # éªŒè¯é›†è¯„ä¼° - è®¡ç®—è¯¦ç»†çš„å›å½’æŒ‡æ ‡ï¼ˆåœ¨åŸå€¼ç©ºé—´ï¼‰
    val_metrics = calculate_regression_metrics(model, val_loader, criterion, device, label_scaler)
    
    return {
        'fold': fold_idx + 1,
        'best_val_loss': best_val_loss,
        'test_loss': test_metrics['loss'],
        'final_train_loss': train_losses[-1] if train_losses else float('inf'),
        'final_val_loss': val_losses[-1] if val_losses else float('inf'),
        'train_losses': train_losses,
        'val_losses': val_losses,
        'model_path': str(output_dir / f'best_model_fold_{fold_idx + 1}.pth'),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²é¿å…JSONåºåˆ—åŒ–é—®é¢˜
        # éªŒè¯é›†æŒ‡æ ‡
        'val_r2': val_metrics['r2'],
        'val_rmse': val_metrics['rmse'],
        'val_mae': val_metrics['mae'],
        'val_mse': val_metrics['mse'],
        'val_pearson': val_metrics['pearson'],
        'val_spearman': val_metrics['spearman'],
        # æµ‹è¯•é›†æŒ‡æ ‡
        'test_r2': test_metrics['r2'],
        'test_rmse': test_metrics['rmse'],
        'test_mae': test_metrics['mae'],
        'test_mse': test_metrics['mse'],
        'test_pearson': test_metrics['pearson'],
        'test_spearman': test_metrics['spearman'],
        # é¢„æµ‹ç»“æœï¼ˆç”¨äºåç»­åˆ†æï¼‰
        'test_predictions': test_metrics['predictions'].tolist(),  # è½¬æ¢ä¸ºåˆ—è¡¨é¿å…JSONåºåˆ—åŒ–é—®é¢˜
        'test_targets': test_metrics['targets'].tolist(),
        'val_predictions': val_metrics['predictions'].tolist(),
        'val_targets': val_metrics['targets'].tolist()
    }

def create_data_splits(data_dict, train_ratio=0.7, val_ratio=0.15):
    """
    åˆ›å»ºæ•°æ®åˆ†å‰² - ä½¿ç”¨åˆ†å±‚é‡‡æ ·ç¡®ä¿æ•°æ®åˆ†å¸ƒå‡è¡¡
    """
    print("åˆ›å»ºæ•°æ®åˆ†å‰²...")
    
    # è·å–æ•°æ®
    targets = data_dict['targets']
    identifiers = data_dict['identifiers']
    
    # è®¡ç®—åˆ†å‰²ç‚¹
    total_samples = len(targets)
    train_size = int(total_samples * train_ratio)
    val_size = int(total_samples * val_ratio)
    test_size = total_samples - train_size - val_size
    
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  è®­ç»ƒé›†: {train_size} ({train_ratio*100:.1f}%)")
    print(f"  éªŒè¯é›†: {val_size} ({val_ratio*100:.1f}%)")
    print(f"  æµ‹è¯•é›†: {test_size} ({(1-train_ratio-val_ratio)*100:.1f}%)")
    
    # ä½¿ç”¨åˆ†å±‚é‡‡æ · - åŸºäºç›®æ ‡å€¼èŒƒå›´åˆ†å±‚
    target_values = targets.squeeze().numpy()
    
    # åˆ›å»ºåˆ†å±‚ç´¢å¼•
    # å°†ç›®æ ‡å€¼åˆ†ä¸ºå‡ ä¸ªåŒºé—´ï¼Œç¡®ä¿æ¯ä¸ªåŒºé—´åœ¨è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†ä¸­éƒ½æœ‰ä»£è¡¨
    num_bins = min(10, len(targets) // 100)  # æ ¹æ®æ•°æ®é‡ç¡®å®šbinæ•°é‡
    if num_bins < 3:
        num_bins = 3
    
    # åˆ›å»ºåˆ†å±‚ç´¢å¼•
    bin_indices = np.linspace(target_values.min(), target_values.max(), num_bins + 1)
    bin_labels = np.digitize(target_values, bin_indices)
    
    # åˆ†å±‚é‡‡æ ·
    train_indices = []
    val_indices = []
    test_indices = []
    
    for bin_id in range(1, num_bins + 1):
        bin_mask = bin_labels == bin_id
        bin_indices_list = np.where(bin_mask)[0]
        
        if len(bin_indices_list) > 0:
            # éšæœºæ‰“ä¹±
            np.random.shuffle(bin_indices_list)
            
            # æŒ‰æ¯”ä¾‹åˆ†é…
            bin_train_size = max(1, int(len(bin_indices_list) * train_ratio))
            bin_val_size = max(1, int(len(bin_indices_list) * val_ratio))
            
            train_indices.extend(bin_indices_list[:bin_train_size])
            val_indices.extend(bin_indices_list[bin_train_size:bin_train_size + bin_val_size])
            test_indices.extend(bin_indices_list[bin_train_size + bin_val_size:])
    
    # ç¡®ä¿ç´¢å¼•æ•°é‡æ­£ç¡®
    train_indices = train_indices[:train_size]
    val_indices = val_indices[:val_size]
    test_indices = test_indices[:test_size]
    
    # å¦‚æœæŸä¸ªé›†åˆæ ·æœ¬ä¸è¶³ï¼Œä»å…¶ä»–é›†åˆè¡¥å……
    while len(train_indices) < train_size:
        remaining = list(set(range(total_samples)) - set(train_indices) - set(val_indices) - set(test_indices))
        if remaining:
            train_indices.append(remaining[0])
        else:
            break
    
    while len(val_indices) < val_size:
        remaining = list(set(range(total_samples)) - set(train_indices) - set(val_indices) - set(test_indices))
        if remaining:
            val_indices.append(remaining[0])
        else:
            break
    
    while len(test_indices) < test_size:
        remaining = list(set(range(total_samples)) - set(train_indices) - set(val_indices) - set(test_indices))
        if remaining:
            test_indices.append(remaining[0])
        else:
            break
    
    print(f"  å®é™…åˆ†å‰²: è®­ç»ƒ={len(train_indices)}, éªŒè¯={len(val_indices)}, æµ‹è¯•={len(test_indices)}")
    
    # éªŒè¯åˆ†å‰²è´¨é‡
    train_targets = targets[train_indices]
    val_targets = targets[val_indices]
    test_targets = targets[test_indices]
    
    print(f"  ç›®æ ‡å€¼åˆ†å¸ƒ:")
    print(f"    è®­ç»ƒé›†: [{train_targets.min():.4f}, {train_targets.max():.4f}], å‡å€¼={train_targets.mean():.4f}")
    print(f"    éªŒè¯é›†: [{val_targets.min():.4f}, {val_targets.max():.4f}], å‡å€¼={val_targets.mean():.4f}")
    print(f"    æµ‹è¯•é›†: [{test_targets.min():.4f}, {test_targets.max():.4f}], å‡å€¼={test_targets.mean():.4f}")
    
    # æ£€æŸ¥åˆ†å¸ƒç›¸ä¼¼æ€§
    train_std = train_targets.std()
    val_std = val_targets.std()
    test_std = test_targets.std()
    
    if abs(train_std - val_std) / train_std > 0.3 or abs(train_std - test_std) / train_std > 0.3:
        print("  âš ï¸ è­¦å‘Š: æ•°æ®é›†é—´æ ‡å‡†å·®å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ³›åŒ–")
    
    def split_data(data_dict, indices):
        """åˆ†å‰²æ•°æ®å­—å…¸"""
        split_dict = {}
        for key, value in data_dict.items():
            if isinstance(value, torch.Tensor):
                split_dict[key] = value[indices]
            elif isinstance(value, list):
                split_dict[key] = [value[i] for i in indices]
            else:
                split_dict[key] = value
        return split_dict
    
    # åˆ›å»ºåˆ†å‰²åçš„æ•°æ®
    train_data = split_data(data_dict, train_indices)
    val_data = split_data(data_dict, val_indices)
    test_data = split_data(data_dict, test_indices)
    
    return train_data, val_data, test_data

def create_cross_validation_splits(data_dict, n_folds=5, random_state=42):
    """
    åˆ›å»ºäº”æŠ˜äº¤å‰éªŒè¯çš„æ•°æ®åˆ†å‰² - ä½¿ç”¨åˆ†å±‚KæŠ˜ç¡®ä¿æ¯æŠ˜æ•°æ®åˆ†å¸ƒå‡è¡¡
    """
    print(f"åˆ›å»º{n_folds}æŠ˜äº¤å‰éªŒè¯æ•°æ®åˆ†å‰²...")
    
    # è·å–æ•°æ®
    targets = data_dict['targets']
    identifiers = data_dict['identifiers']
    target_values = targets.squeeze().numpy()
    
    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    np.random.seed(random_state)
    
    # ä½¿ç”¨åˆ†å±‚KæŠ˜åˆ†å‰²
    from sklearn.model_selection import StratifiedKFold
    
    # åˆ›å»ºåˆ†å±‚æ ‡ç­¾ - åŸºäºç›®æ ‡å€¼åˆ†ä½æ•°
    n_bins = min(10, len(targets) // 50)  # æ ¹æ®æ•°æ®é‡ç¡®å®šbinæ•°é‡
    if n_bins < 5:
        n_bins = 5
    
    # åˆ›å»ºåˆ†å±‚æ ‡ç­¾
    bin_edges = np.quantile(target_values, np.linspace(0, 1, n_bins + 1))
    bin_labels = np.digitize(target_values, bin_edges) - 1
    bin_labels = np.clip(bin_labels, 0, n_bins - 1)
    
    # åˆ›å»ºåˆ†å±‚KæŠ˜åˆ†å‰²å™¨
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    
    cv_splits = []
    
    for fold_idx, (train_val_indices, test_indices) in enumerate(skf.split(range(len(targets)), bin_labels)):
        print(f"  ç¬¬{fold_idx + 1}æŠ˜:")
        
        # å°†è®­ç»ƒ+éªŒè¯é›†è¿›ä¸€æ­¥åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›† (8:2)
        train_val_targets = target_values[train_val_indices]
        train_val_bin_labels = bin_labels[train_val_indices]
        
        # å¯¹è®­ç»ƒ+éªŒè¯é›†å†æ¬¡åˆ†å±‚åˆ†å‰²
        train_val_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state + fold_idx)
        train_indices_fold, val_indices_fold = next(iter(train_val_skf.split(train_val_indices, train_val_bin_labels)))
        
        # è½¬æ¢ä¸ºåŸå§‹ç´¢å¼•
        train_indices = train_val_indices[train_indices_fold]
        val_indices = train_val_indices[val_indices_fold]
        
        # ç»Ÿè®¡ä¿¡æ¯
        train_targets = targets[train_indices]
        val_targets = targets[val_indices]
        test_targets = targets[test_indices]
        
        print(f"    è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬")
        print(f"    éªŒè¯é›†: {len(val_indices)} æ ·æœ¬")
        print(f"    æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬")
        print(f"    ç›®æ ‡å€¼èŒƒå›´ - è®­ç»ƒ:[{train_targets.min():.4f}, {train_targets.max():.4f}], "
              f"éªŒè¯:[{val_targets.min():.4f}, {val_targets.max():.4f}], "
              f"æµ‹è¯•:[{test_targets.min():.4f}, {test_targets.max():.4f}]")
        
        # æ£€æŸ¥åˆ†å¸ƒç›¸ä¼¼æ€§
        train_std = train_targets.std()
        val_std = val_targets.std()
        test_std = test_targets.std()
        
        if abs(train_std - val_std) / train_std > 0.4 or abs(train_std - test_std) / train_std > 0.4:
            print(f"    âš ï¸ è­¦å‘Š: ç¬¬{fold_idx + 1}æŠ˜æ•°æ®åˆ†å¸ƒå·®å¼‚è¾ƒå¤§")
        
        cv_splits.append({
            'fold': fold_idx + 1,
            'train_indices': train_indices,
            'val_indices': val_indices,
            'test_indices': test_indices
        })
    
    return cv_splits

def split_data_by_indices(data_dict, indices):
    """æ ¹æ®ç´¢å¼•åˆ†å‰²æ•°æ®å­—å…¸"""
    split_dict = {}
    for key, value in data_dict.items():
        if isinstance(value, torch.Tensor):
            split_dict[key] = value[indices]
        elif isinstance(value, list):
            split_dict[key] = [value[i] for i in indices]
        else:
            split_dict[key] = value
    return split_dict

def run_cross_validation(data_dict, args, device, output_dir):
    """
    è¿è¡Œäº”æŠ˜äº¤å‰éªŒè¯
    """
    print("="*80)
    print("å¼€å§‹äº”æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ")
    print("="*80)
    
    # åˆ›å»ºäº¤å‰éªŒè¯åˆ†å‰²
    cv_splits = create_cross_validation_splits(data_dict, n_folds=5, random_state=42)
    
    # å­˜å‚¨æ¯æŠ˜çš„ç»“æœ
    fold_results = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æè¿°ç¬¦
    use_descriptors = data_dict['descriptors'] is not None
    descriptor_dim = data_dict['descriptors'].shape[1] if use_descriptors else 0
    
    # è·å–æ ‡ç­¾æ ‡å‡†åŒ–å™¨
    label_scaler = data_dict.get('label_scaler', None)
    if label_scaler is not None:
        print(f"âœ… å·²è·å–æ ‡ç­¾æ ‡å‡†åŒ–å™¨ï¼Œå°†åœ¨åŸå€¼ç©ºé—´è®¡ç®—æ€§èƒ½æŒ‡æ ‡")
        print(f"  åŸå§‹æ ‡ç­¾å‡å€¼: {label_scaler.mean_[0]:.4f}")
        print(f"  åŸå§‹æ ‡ç­¾æ ‡å‡†å·®: {label_scaler.scale_[0]:.4f}")
    else:
        # ä¸å†åŸºäºå·²æ ‡å‡†åŒ–çš„targetsäºŒæ¬¡æ‹Ÿåˆæ ‡å‡†åŒ–å™¨ï¼Œç›´æ¥åœ¨æ ‡å‡†åŒ–ç©ºé—´è®¡ç®—å¹¶ç»™å‡ºæ˜ç¡®æç¤º
        print("âš ï¸ æœªè·å–åˆ°æ ‡ç­¾æ ‡å‡†åŒ–å™¨ï¼Œå°†åœ¨æ ‡å‡†åŒ–ç©ºé—´è®¡ç®—æŒ‡æ ‡ï¼ˆä¸è¿›è¡ŒäºŒæ¬¡æ‹Ÿåˆï¼‰")
    
    # VAEé…ç½®
    vae_config = {
        'latent_dim': 128,
        'num_planes': 9,
        'dropout_rate': args.dropout_rate
    }
    
    # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    model_paths = {
        'vae': args.vae_model_path,
        'gcn_cc': args.gcn_cc_model_path,
        'gcn_noncc': args.gcn_noncc_model_path,
        'lzhnn': args.lzhnn_model_path
    }
    
    for fold_idx, split in enumerate(cv_splits):
        print(f"\nğŸ”„ å¤„ç†ç¬¬{fold_idx + 1}/5æŠ˜...")
        
        # åˆ›å»ºè¯¥æŠ˜çš„æ•°æ®
        train_data = split_data_by_indices(data_dict, split['train_indices'])
        val_data = split_data_by_indices(data_dict, split['val_indices'])
        test_data = split_data_by_indices(data_dict, split['test_indices'])
        
        # åˆ›å»ºæ–°æ¨¡å‹ï¼ˆæ¯æŠ˜ä½¿ç”¨ç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹ï¼‰
        model = create_exact_fusion_model(
            vae_config=vae_config,
            use_descriptors=use_descriptors,
            descriptor_dim=descriptor_dim
        ).to(device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        model.load_pretrained_weights(model_paths)
        
        # è®­ç»ƒè¯¥æŠ˜
        fold_result = train_single_fold(
            model, train_data, val_data, test_data, 
            args, device, fold_idx, output_dir, label_scaler
        )
        
        fold_results.append(fold_result)
        
        # æ‰“å°è¯¥æŠ˜ç»“æœ
        print(f"ç¬¬{fold_idx + 1}æŠ˜ç»“æœ:")
        print(f"  éªŒè¯é›† - æŸå¤±: {fold_result['best_val_loss']:.4f}, RÂ²: {fold_result['val_r2']:.4f}, RMSE: {fold_result['val_rmse']:.4f}, MAE: {fold_result['val_mae']:.4f}, Pearson: {fold_result['val_pearson']:.4f}, Spearman: {fold_result['val_spearman']:.4f}")
        print(f"  æµ‹è¯•é›† - æŸå¤±: {fold_result['test_loss']:.4f}, RÂ²: {fold_result['test_r2']:.4f}, RMSE: {fold_result['test_rmse']:.4f}, MAE: {fold_result['test_mae']:.4f}, Pearson: {fold_result['test_pearson']:.4f}, Spearman: {fold_result['test_spearman']:.4f}")
        
        # æ¸…ç†GPUå†…å­˜
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    return fold_results

def aggregate_cv_results(fold_results, output_dir):
    """
    èšåˆäº¤å‰éªŒè¯ç»“æœå¹¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    """
    print("\n" + "="*80)
    print("äº¤å‰éªŒè¯ç»“æœèšåˆ (åŸå€¼ç©ºé—´)")
    print("="*80)
    
    # æå–å…³é”®æŒ‡æ ‡
    val_losses = [result['best_val_loss'] for result in fold_results]
    test_losses = [result['test_loss'] for result in fold_results]
    final_train_losses = [result['final_train_loss'] for result in fold_results]
    final_val_losses = [result['final_val_loss'] for result in fold_results]
    
    # æå–å›å½’æŒ‡æ ‡
    val_r2_scores = [result['val_r2'] for result in fold_results]
    test_r2_scores = [result['test_r2'] for result in fold_results]
    val_rmse_scores = [result['val_rmse'] for result in fold_results]
    test_rmse_scores = [result['test_rmse'] for result in fold_results]
    val_mae_scores = [result['val_mae'] for result in fold_results]
    test_mae_scores = [result['test_mae'] for result in fold_results]
    val_mse_scores = [result['val_mse'] for result in fold_results]
    test_mse_scores = [result['test_mse'] for result in fold_results]
    # æ–°å¢ç›¸å…³æ€§æŒ‡æ ‡
    val_pearson_scores = [result.get('val_pearson', float('nan')) for result in fold_results]
    test_pearson_scores = [result.get('test_pearson', float('nan')) for result in fold_results]
    val_spearman_scores = [result.get('val_spearman', float('nan')) for result in fold_results]
    test_spearman_scores = [result.get('test_spearman', float('nan')) for result in fold_results]
    
    # è®¡ç®—ç»Ÿè®¡é‡
    import statistics
    
    def calculate_stats(values):
        return {
            'mean': statistics.mean(values),
            'std': statistics.stdev(values) if len(values) > 1 else 0,
            'min': min(values),
            'max': max(values),
            'values': values
        }
    
    stats = {
        'val_loss': calculate_stats(val_losses),
        'test_loss': calculate_stats(test_losses),
        'final_train_loss': calculate_stats(final_train_losses),
        'final_val_loss': calculate_stats(final_val_losses),
        # éªŒè¯é›†å›å½’æŒ‡æ ‡
        'val_r2': calculate_stats(val_r2_scores),
        'val_rmse': calculate_stats(val_rmse_scores),
        'val_mae': calculate_stats(val_mae_scores),
        'val_mse': calculate_stats(val_mse_scores),
        'val_pearson': calculate_stats(val_pearson_scores),
        'val_spearman': calculate_stats(val_spearman_scores),
        # æµ‹è¯•é›†å›å½’æŒ‡æ ‡
        'test_r2': calculate_stats(test_r2_scores),
        'test_rmse': calculate_stats(test_rmse_scores),
        'test_mae': calculate_stats(test_mae_scores),
        'test_mse': calculate_stats(test_mse_scores),
        'test_pearson': calculate_stats(test_pearson_scores),
        'test_spearman': calculate_stats(test_spearman_scores)
    }
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("ğŸ“Š äº¤å‰éªŒè¯ç»Ÿè®¡ç»“æœ:")
    print("-" * 70)
    
    # æŸå¤±ç»Ÿè®¡
    print("ğŸ”¸ æŸå¤±æŒ‡æ ‡:")
    print(f"  éªŒè¯æŸå¤±: {stats['val_loss']['mean']:.4f} Â± {stats['val_loss']['std']:.4f}")
    print(f"    èŒƒå›´: [{stats['val_loss']['min']:.4f}, {stats['val_loss']['max']:.4f}]")
    print(f"  æµ‹è¯•æŸå¤±: {stats['test_loss']['mean']:.4f} Â± {stats['test_loss']['std']:.4f}")
    print(f"    èŒƒå›´: [{stats['test_loss']['min']:.4f}, {stats['test_loss']['max']:.4f}]")
    
    # éªŒè¯é›†å›å½’æŒ‡æ ‡
    print("\nğŸ”¸ éªŒè¯é›†å›å½’æŒ‡æ ‡:")
    print(f"  RÂ²: {stats['val_r2']['mean']:.4f} Â± {stats['val_r2']['std']:.4f}")
    print(f"    èŒƒå›´: [{stats['val_r2']['min']:.4f}, {stats['val_r2']['max']:.4f}]")
    print(f"  RMSE: {stats['val_rmse']['mean']:.4f} Â± {stats['val_rmse']['std']:.4f}")
    print(f"    èŒƒå›´: [{stats['val_rmse']['min']:.4f}, {stats['val_rmse']['max']:.4f}]")
    print(f"  MAE: {stats['val_mae']['mean']:.4f} Â± {stats['val_mae']['std']:.4f}")
    print(f"    èŒƒå›´: [{stats['val_mae']['min']:.4f}, {stats['val_mae']['max']:.4f}]")
    print(f"  Pearson: {stats['val_pearson']['mean']:.4f} Â± {stats['val_pearson']['std']:.4f}")
    print(f"  Spearman: {stats['val_spearman']['mean']:.4f} Â± {stats['val_spearman']['std']:.4f}")
    
    # æµ‹è¯•é›†å›å½’æŒ‡æ ‡
    print("\nğŸ”¸ æµ‹è¯•é›†å›å½’æŒ‡æ ‡:")
    print(f"  RÂ²: {stats['test_r2']['mean']:.4f} Â± {stats['test_r2']['std']:.4f}")
    print(f"    èŒƒå›´: [{stats['test_r2']['min']:.4f}, {stats['test_r2']['max']:.4f}]")
    print(f"  RMSE: {stats['test_rmse']['mean']:.4f} Â± {stats['test_rmse']['std']:.4f}")
    print(f"    èŒƒå›´: [{stats['test_rmse']['min']:.4f}, {stats['test_rmse']['max']:.4f}]")
    print(f"  MAE: {stats['test_mae']['mean']:.4f} Â± {stats['test_mae']['std']:.4f}")
    print(f"    èŒƒå›´: [{stats['test_mae']['min']:.4f}, {stats['test_mae']['max']:.4f}]")
    print(f"  Pearson: {stats['test_pearson']['mean']:.4f} Â± {stats['test_pearson']['std']:.4f}")
    print(f"  Spearman: {stats['test_spearman']['mean']:.4f} Â± {stats['test_spearman']['std']:.4f}")
    
    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
    val_loss_cv = stats['val_loss']['std'] / stats['val_loss']['mean'] if stats['val_loss']['mean'] > 0 else 0
    test_loss_cv = stats['test_loss']['std'] / stats['test_loss']['mean'] if stats['test_loss']['mean'] > 0 else 0
    val_r2_cv = stats['val_r2']['std'] / abs(stats['val_r2']['mean']) if stats['val_r2']['mean'] != 0 else 0
    test_r2_cv = stats['test_r2']['std'] / abs(stats['test_r2']['mean']) if stats['test_r2']['mean'] != 0 else 0
    
    print(f"\nğŸ¯ æ¨¡å‹ç¨³å®šæ€§:")
    print(f"éªŒè¯æŸå¤±å˜å¼‚ç³»æ•°: {val_loss_cv:.4f}")
    print(f"æµ‹è¯•æŸå¤±å˜å¼‚ç³»æ•°: {test_loss_cv:.4f}")
    print(f"éªŒè¯RÂ²å˜å¼‚ç³»æ•°: {val_r2_cv:.4f}")
    print(f"æµ‹è¯•RÂ²å˜å¼‚ç³»æ•°: {test_r2_cv:.4f}")
    
    # ç»¼åˆç¨³å®šæ€§è¯„ä¼°ï¼ˆåŸºäºRÂ²çš„ç¨³å®šæ€§ï¼‰
    if val_r2_cv < 0.1:
        stability_level = "ğŸŒŸ éå¸¸ç¨³å®š"
    elif val_r2_cv < 0.2:
        stability_level = "âœ… ç¨³å®š"
    elif val_r2_cv < 0.3:
        stability_level = "âš ï¸ ä¸€èˆ¬ç¨³å®š"
    else:
        stability_level = "âŒ ä¸ç¨³å®š"
    
    print(f"æ•´ä½“ç¨³å®šæ€§: {stability_level}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    import json
    import pandas as pd
    
    # ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœ
    results_file = output_dir / 'cross_validation_results.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'fold_results': fold_results,
            'statistics': stats,
            'stability': {
                'val_loss_cv': val_loss_cv,
                'test_loss_cv': test_loss_cv,
                'val_r2_cv': val_r2_cv,
                'test_r2_cv': test_r2_cv,
                'stability_level': stability_level
            }
        }, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜CSVæ ¼å¼çš„æ‘˜è¦
    summary_data = []
    for i, result in enumerate(fold_results):
        summary_data.append({
            'fold': result['fold'],
            'best_val_loss': result['best_val_loss'],
            'test_loss': result['test_loss'],
            'val_r2': result['val_r2'],
            'test_r2': result['test_r2'],
            'val_rmse': result['val_rmse'],
            'test_rmse': result['test_rmse'],
            'val_mae': result['val_mae'],
            'test_mae': result['test_mae'],
            'val_pearson': result.get('val_pearson', float('nan')),
            'test_pearson': result.get('test_pearson', float('nan')),
            'val_spearman': result.get('val_spearman', float('nan')),
            'test_spearman': result.get('test_spearman', float('nan')),
            'final_train_loss': result['final_train_loss'],
            'final_val_loss': result['final_val_loss']
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_file = output_dir / 'cross_validation_summary.csv'
    summary_df.to_csv(summary_file, index=False, encoding='utf-8')
    
    # æ·»åŠ ç»Ÿè®¡è¡Œ
    stats_row = pd.DataFrame([{
        'fold': 'Mean',
        'best_val_loss': stats['val_loss']['mean'],
        'test_loss': stats['test_loss']['mean'],
        'val_r2': stats['val_r2']['mean'],
        'test_r2': stats['test_r2']['mean'],
        'val_rmse': stats['val_rmse']['mean'],
        'test_rmse': stats['test_rmse']['mean'],
        'val_mae': stats['val_mae']['mean'],
        'test_mae': stats['test_mae']['mean'],
        'val_pearson': stats['val_pearson']['mean'],
        'test_pearson': stats['test_pearson']['mean'],
        'val_spearman': stats['val_spearman']['mean'],
        'test_spearman': stats['test_spearman']['mean'],
        'final_train_loss': stats['final_train_loss']['mean'],
        'final_val_loss': stats['final_val_loss']['mean']
    }])
    
    std_row = pd.DataFrame([{
        'fold': 'Std',
        'best_val_loss': stats['val_loss']['std'],
        'test_loss': stats['test_loss']['std'],
        'val_r2': stats['val_r2']['std'],
        'test_r2': stats['test_r2']['std'],
        'val_rmse': stats['val_rmse']['std'],
        'test_rmse': stats['test_rmse']['std'],
        'val_mae': stats['val_mae']['std'],
        'test_mae': stats['test_mae']['std'],
        'val_pearson': stats['val_pearson']['std'],
        'test_pearson': stats['test_pearson']['std'],
        'val_spearman': stats['val_spearman']['std'],
        'test_spearman': stats['test_spearman']['std'],
        'final_train_loss': stats['final_train_loss']['std'],
        'final_val_loss': stats['final_val_loss']['std']
    }])
    
    final_summary = pd.concat([summary_df, stats_row, std_row], ignore_index=True)
    final_summary.to_csv(summary_file, index=False, encoding='utf-8')
    
    print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜:")
    print(f"  è¯¦ç»†ç»“æœ: {results_file}")
    print(f"  æ‘˜è¦è¡¨æ ¼: {summary_file}")
    
    return stats, fold_results

def main():
    # è§£æå‚æ•°
    parser = argparse.ArgumentParser(description='äº¤å‰æ³¨æ„åŠ›èåˆè®­ç»ƒ')
    parser.add_argument('--vae_model_path', type=str, default='PP-cVAE/models/best_model_CH401.pth')
    parser.add_argument('--gcn_cc_model_path', type=str, default='BiG-CAE/models/contrastive_model_cc_CH401.pth')
    parser.add_argument('--gcn_noncc_model_path', type=str, default='BiG-CAE/models/contrastive_model_noncc_CH401.pth')
    parser.add_argument('--lzhnn_model_path', type=str, default='PP-NN/final_model_42_CH401.pth')
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--num_epochs', type=int, default=50)
    parser.add_argument('--learning_rate', type=float, default=1e-4)
    parser.add_argument('--max_samples', type=int, default=1000)
    parser.add_argument('--output_dir', type=str, default='exact_fusion_output')
    parser.add_argument('--weight_decay', type=float, default=1e-4)  # å¢åŠ æƒé‡è¡°å‡
    parser.add_argument('--dropout_rate', type=float, default=0.3)   # å¢åŠ dropout
    parser.add_argument('--early_stopping_patience', type=int, default=15)  # å¢åŠ æ—©åœè€å¿ƒå€¼
    parser.add_argument('--use_cross_validation', action='store_true', help='ä½¿ç”¨äº”æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ')
    parser.add_argument('--n_folds', type=int, default=5, help='äº¤å‰éªŒè¯æŠ˜æ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # === 1. åŸºäºåŸå§‹ä»£ç åŠ è½½æ•°æ® ===
    print("åŸºäºåŸå§‹ä»£ç åŠ è½½æ•°æ®...")
    data_loader = create_exact_data_loader(max_samples=args.max_samples)
    data_dict = data_loader.load_all_data()
    
    # æ•°æ®éªŒè¯ - æ£€æŸ¥æ•°æ®è´¨é‡
    print("\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥...")
    targets = data_dict['targets']
    print(f"  ç›®æ ‡å€¼ç»Ÿè®¡:")
    print(f"    èŒƒå›´: [{targets.min():.4f}, {targets.max():.4f}]")
    print(f"    å‡å€¼: {targets.mean():.4f}")
    print(f"    æ ‡å‡†å·®: {targets.std():.4f}")
    print(f"    æ ·æœ¬æ•°: {len(targets)}")
    
    # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
    if targets.std() < 0.1:
        print("  âš ï¸ è­¦å‘Š: ç›®æ ‡å€¼æ ‡å‡†å·®è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®é—®é¢˜")
    
    # æ ¹æ®å‚æ•°é€‰æ‹©è®­ç»ƒæ¨¡å¼
    if args.use_cross_validation:
        print(f"\nğŸ”„ ä½¿ç”¨{args.n_folds}æŠ˜äº¤å‰éªŒè¯æ¨¡å¼")
        
        # è¿è¡Œäº¤å‰éªŒè¯
        fold_results = run_cross_validation(data_dict, args, device, output_dir)
        
        # èšåˆç»“æœ
        stats, _ = aggregate_cv_results(fold_results, output_dir)
        
    # æ‰“å°æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ‰ äº”æŠ˜äº¤å‰éªŒè¯è®­ç»ƒå®Œæˆ!")
    print("="*80)
    
    print("ğŸ“Š æœ€ç»ˆæ€§èƒ½æ€»ç»“ (åŸå€¼ç©ºé—´):")
    print(f"  éªŒè¯é›† RÂ²: {stats['val_r2']['mean']:.4f} Â± {stats['val_r2']['std']:.4f}")
    print(f"  æµ‹è¯•é›† RÂ²: {stats['test_r2']['mean']:.4f} Â± {stats['test_r2']['std']:.4f}")
    print(f"  éªŒè¯é›† RMSE: {stats['val_rmse']['mean']:.4f} Â± {stats['val_rmse']['std']:.4f}")
    print(f"  æµ‹è¯•é›† RMSE: {stats['test_rmse']['mean']:.4f} Â± {stats['test_rmse']['std']:.4f}")
    print(f"  éªŒè¯é›† MAE: {stats['val_mae']['mean']:.4f} Â± {stats['val_mae']['std']:.4f}")
    print(f"  æµ‹è¯•é›† MAE: {stats['test_mae']['mean']:.4f} Â± {stats['test_mae']['std']:.4f}")
    
    print(f"\nğŸ“ˆ æŸå¤±æŒ‡æ ‡:")
    print(f"  å¹³å‡éªŒè¯æŸå¤±: {stats['val_loss']['mean']:.4f} Â± {stats['val_loss']['std']:.4f}")
    print(f"  å¹³å‡æµ‹è¯•æŸå¤±: {stats['test_loss']['mean']:.4f} Â± {stats['test_loss']['std']:.4f}")
    
    # è®¡ç®—ç¨³å®šæ€§ï¼ˆåŸºäºRÂ²ï¼‰
    val_r2_cv = stats['val_r2']['std'] / abs(stats['val_r2']['mean']) if stats['val_r2']['mean'] != 0 else 0
    if val_r2_cv < 0.1:
        stability_level = "ğŸŒŸ éå¸¸ç¨³å®š"
    elif val_r2_cv < 0.2:
        stability_level = "âœ… ç¨³å®š"
    elif val_r2_cv < 0.3:
        stability_level = "âš ï¸ ä¸€èˆ¬ç¨³å®š"
    else:
        stability_level = "âŒ ä¸ç¨³å®š"
    
    print(f"\nğŸ¯ æ¨¡å‹ç¨³å®šæ€§: {stability_level} (RÂ²å˜å¼‚ç³»æ•°: {val_r2_cv:.4f})")
    print(f"ğŸ“ ç»“æœä¿å­˜è·¯å¾„: {output_dir}")
    return
    
    # å¸¸è§„å•æ¬¡è®­ç»ƒæ¨¡å¼
    print(f"\nğŸ”„ ä½¿ç”¨å¸¸è§„å•æ¬¡è®­ç»ƒæ¨¡å¼")
    
    # æ•°æ®åˆ†å‰²
    train_data, val_data, test_data = create_data_splits(data_dict)
    
    # éªŒè¯åˆ†å‰²è´¨é‡
    train_targets = train_data['targets']
    val_targets = val_data['targets']
    test_targets = test_data['targets']
    
    print(f"  æ•°æ®åˆ†å‰²ç»Ÿè®¡:")
    print(f"    è®­ç»ƒé›†: {len(train_targets)} æ ·æœ¬, RÂ²èŒƒå›´: [{train_targets.min():.4f}, {train_targets.max():.4f}]")
    print(f"    éªŒè¯é›†: {len(val_targets)} æ ·æœ¬, RÂ²èŒƒå›´: [{val_targets.min():.4f}, {val_targets.max():.4f}]")
    print(f"    æµ‹è¯•é›†: {len(test_targets)} æ ·æœ¬, RÂ²èŒƒå›´: [{test_targets.min():.4f}, {test_targets.max():.4f}]")
    
    # === 2. åŸºäºåŸå§‹VAEé…ç½®åˆ›å»ºæ¨¡å‹ ===
    print("\nåˆ›å»ºèåˆæ¨¡å‹...")
    vae_config = {
        'latent_dim': 128,
        'num_planes': 9,
        'dropout_rate': args.dropout_rate  # ä½¿ç”¨å‚æ•°åŒ–çš„dropout
    }
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æè¿°ç¬¦
    use_descriptors = data_dict['descriptors'] is not None
    descriptor_dim = data_dict['descriptors'].shape[1] if use_descriptors else 0
    
    model = create_exact_fusion_model(
        vae_config=vae_config,
        use_descriptors=use_descriptors,
        descriptor_dim=descriptor_dim
    ).to(device)
    
    # === 3. åŸºäºåŸå§‹ä¿å­˜æ ¼å¼åŠ è½½é¢„è®­ç»ƒæƒé‡ ===
    print("åŠ è½½é¢„è®­ç»ƒæƒé‡...")
    model_paths = {
        'vae': args.vae_model_path,
        'gcn_cc': args.gcn_cc_model_path,
        'gcn_noncc': args.gcn_noncc_model_path,
        'lzhnn': args.lzhnn_model_path
    }
    model.load_pretrained_weights(model_paths)
    
    # === 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨ ===
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_dataset = ExactFusionDataset(train_data)
    val_dataset = ExactFusionDataset(val_data)
    test_dataset = ExactFusionDataset(test_data)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        collate_fn=exact_collate_fn
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        collate_fn=exact_collate_fn
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        collate_fn=exact_collate_fn
    )
    
    # === 5. è®­ç»ƒé…ç½® ===
    criterion = ExactFusionLoss()
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=args.learning_rate,
        weight_decay=args.weight_decay  # ä½¿ç”¨å‚æ•°åŒ–çš„æƒé‡è¡°å‡
    )
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=8, factor=0.7, min_lr=1e-6  # è°ƒæ•´å­¦ä¹ ç‡è°ƒåº¦
    )
    early_stopping = ExactEarlyStopping(patience=args.early_stopping_patience)
    
    # æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print("=" * 60)
    print("èåˆæ¨¡å‹ä¿¡æ¯")
    print("=" * 60)
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"å†»ç»“å‚æ•°: {total_params - trainable_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {trainable_params/total_params*100:.2f}%")
    
    # è·å–å½’ä¸€åŒ–åçš„æƒé‡ä¿¡æ¯
    main_weight, fusion_weight = model.get_normalized_weights()
    print(f"ä¸»æ¨¡å‹æƒé‡: {main_weight.item():.2f}")
    print(f"èåˆæ¨¡å‹æƒé‡: {fusion_weight.item():.2f}")
    print(f"æƒé‡è¡°å‡: {args.weight_decay}")
    print(f"Dropoutç‡: {args.dropout_rate}")
    print("=" * 60)
    
    # === 6. è®­ç»ƒå¾ªç¯ ===
    print("å¼€å§‹è®­ç»ƒ...")
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []
    
    for epoch in range(args.num_epochs):
        print(f"\nEpoch {epoch+1}/{args.num_epochs}")
        print("-" * 50)
        
        # è®­ç»ƒ
        train_loss = exact_train_epoch(model, train_loader, criterion, optimizer, device)
        train_losses.append(train_loss)
        print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        
        # éªŒè¯
        val_loss = exact_validate_epoch(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        print(f"éªŒè¯æŸå¤±: {val_loss:.4f}")
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
        
        # æ£€æŸ¥è¿‡æ‹Ÿåˆ
        if len(train_losses) > 5:
            train_trend = sum(train_losses[-5:]) / 5
            val_trend = sum(val_losses[-5:]) / 5
            if val_trend > train_trend * 1.2:  # éªŒè¯æŸå¤±æ¯”è®­ç»ƒæŸå¤±é«˜20%
                print("  âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆè¶‹åŠ¿ï¼Œè€ƒè™‘è°ƒæ•´æ­£åˆ™åŒ–å‚æ•°")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model_save_path = output_dir / 'best_fusion_model.pth'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'train_loss': train_loss,
                'config': {
                    'vae_config': vae_config,
                    'use_descriptors': use_descriptors,
                    'descriptor_dim': descriptor_dim,
                    'weight_decay': args.weight_decay,
                    'dropout_rate': args.dropout_rate
                }
            }, model_save_path)
            print(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {model_save_path}")
        
        # Early stopping
        if early_stopping(val_loss):
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    # === 7. æµ‹è¯•è¯„ä¼° ===
    print("\n" + "=" * 60)
    print("æµ‹è¯•è¯„ä¼°")
    print("=" * 60)
    
    test_loss = exact_validate_epoch(model, test_loader, criterion, device)
    print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    
    # è®­ç»ƒå†å²åˆ†æ
    print(f"\nğŸ“Š è®­ç»ƒå†å²åˆ†æ:")
    print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
    print(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.4f}")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    if len(train_losses) > 1:
        train_improvement = (train_losses[0] - train_losses[-1]) / train_losses[0] * 100
        val_improvement = (val_losses[0] - val_losses[-1]) / val_losses[0] * 100
        print(f"  è®­ç»ƒæŸå¤±æ”¹å–„: {train_improvement:.2f}%")
        print(f"  éªŒè¯æŸå¤±æ”¹å–„: {val_improvement:.2f}%")
    
    # ===== è¯¦ç»†æ€§èƒ½è¯„ä¼° =====
    print("\n" + "="*70)
    print("ğŸ” å¼€å§‹è¯¦ç»†æ€§èƒ½è¯„ä¼°...")
    print("="*70)
    
    try:
        from fusion_model_evaluator import create_comprehensive_evaluation_pipeline
        
        # åˆ›å»ºè¯„ä¼°ç»“æœç›®å½•
        eval_output_dir = output_dir / "evaluation_results"
        eval_output_dir.mkdir(exist_ok=True)
        
        # æ‰§è¡Œå…¨é¢è¯„ä¼°
        evaluator, eval_results = create_comprehensive_evaluation_pipeline(
            model=model,
            test_loader=test_loader,
            val_loader=val_loader,
            individual_models=None,  # å¯ä»¥åç»­æ·»åŠ å•ç‹¬æ¨¡å‹å¯¹æ¯”
            save_dir=str(eval_output_dir)
        )
        
        print("\nğŸ“ˆ è¯„ä¼°ç»“æœæ‘˜è¦:")
        if 'test' in eval_results:
            test_metrics = eval_results['test']['regression_metrics']
            print(f"  â€¢ æµ‹è¯•é›† RÂ²: {test_metrics['RÂ²']:.4f}")
            print(f"  â€¢ æµ‹è¯•é›† RMSE: {test_metrics['RMSE']:.4f}")
            print(f"  â€¢ æµ‹è¯•é›† MAE: {test_metrics['MAE']:.4f}")
            print(f"  â€¢ æµ‹è¯•é›† MAPE: {test_metrics['MAPE']:.2f}%")
            
            # æ¨ç†æ€§èƒ½
            if 'inference_stats' in eval_results['test']:
                inf_stats = eval_results['test']['inference_stats']
                print(f"  â€¢ æ¨ç†é€Ÿåº¦: {inf_stats['samples_per_second']:.1f} samples/s")
                print(f"  â€¢ å¹³å‡æ¨ç†æ—¶é—´: {inf_stats['avg_inference_time']*1000:.2f} ms")
        
        if 'validation' in eval_results:
            val_metrics = eval_results['validation']['regression_metrics']
            print(f"  â€¢ éªŒè¯é›† RÂ²: {val_metrics['RÂ²']:.4f}")
            print(f"  â€¢ éªŒè¯é›† RMSE: {val_metrics['RMSE']:.4f}")
            
            # æ³›åŒ–æ€§èƒ½åˆ†æ
            test_r2 = eval_results['test']['regression_metrics']['RÂ²']
            val_r2 = val_metrics['RÂ²']
            generalization_gap = abs(test_r2 - val_r2)
            if generalization_gap < 0.05:
                print(f"  â€¢ æ³›åŒ–æ€§èƒ½: âœ… è‰¯å¥½ (å·®å¼‚: {generalization_gap:.4f})")
            else:
                print(f"  â€¢ æ³›åŒ–æ€§èƒ½: âš ï¸ éœ€è¦å…³æ³¨ (å·®å¼‚: {generalization_gap:.4f})")
        
        print(f"\nğŸ“‹ è¯¦ç»†è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {eval_output_dir}")
        print("  ğŸ“„ evaluation_report.md - å®Œæ•´è¯„ä¼°æŠ¥å‘Š")
        print("  ğŸ“Š performance_summary.csv - æ€§èƒ½æ‘˜è¦è¡¨")
        print("  ğŸ¯ predictions_vs_targets.png - é¢„æµ‹æ•£ç‚¹å›¾")
        print("  ğŸ“ˆ error_distribution.png - è¯¯å·®åˆ†å¸ƒå›¾")
        print("  ğŸ•¸ï¸ performance_radar.png - æ€§èƒ½é›·è¾¾å›¾")
        print("  ğŸ“‹ test_predictions.csv - æµ‹è¯•é›†é¢„æµ‹ç»“æœ")
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        test_r2 = eval_results['test']['regression_metrics']['RÂ²']
        if test_r2 > 0.9:
            performance_level = "ğŸŒŸ ä¼˜ç§€"
        elif test_r2 > 0.8:
            performance_level = "âœ… è‰¯å¥½"
        elif test_r2 > 0.7:
            performance_level = "âš ï¸ ä¸€èˆ¬"
        else:
            performance_level = "âŒ éœ€è¦æ”¹è¿›"
        
        print(f"\nğŸ† æ¨¡å‹æ€§èƒ½ç­‰çº§: {performance_level} (RÂ² = {test_r2:.4f})")
        
    except Exception as e:
        print(f"âš ï¸ æ€§èƒ½è¯„ä¼°å¤±è´¥: {e}")
        print("è®­ç»ƒå·²å®Œæˆï¼Œä½†è·³è¿‡è¯¦ç»†è¯„ä¼°")
        import traceback
        traceback.print_exc()
    
    print(f"\nâœ… åŸºäºåŸå§‹ä»£ç çš„äº¤å‰æ³¨æ„åŠ›èåˆè®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {output_dir}")

if __name__ == "__main__":
    main() 