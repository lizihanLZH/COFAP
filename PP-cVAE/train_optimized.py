"""ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬ - æé«˜GPUåˆ©ç”¨ç‡"""

import torch
import os
import warnings
warnings.filterwarnings('ignore')

from config import Config
from dataset import create_data_loaders
from models import create_model, count_parameters
from trainer import Trainer
from utils import set_seed

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # é…ç½®
    config = Config()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.SEED)
    
    print("=== ä¼˜åŒ–è®­ç»ƒé…ç½® ===")
    print(f"è®¾å¤‡: {config.DEVICE}")
    print(f"æ‰¹æ¬¡å¤§å°: {config.BATCH_SIZE}")
    print(f"å·¥ä½œè¿›ç¨‹æ•°: {config.NUM_WORKERS}")
    print(f"æ··åˆç²¾åº¦è®­ç»ƒ: {config.USE_MIXED_PRECISION}")
    print(f"æ¨¡å‹ç¼–è¯‘: {getattr(config, 'COMPILE_MODEL', False)}")
    print(f"Channels Last: {getattr(config, 'CHANNELS_LAST', False)}")
    print(f"Pin Memory: {getattr(config, 'PIN_MEMORY', False)}")
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        print(f"\nGPUä¿¡æ¯:")
        print(f"  GPUåç§°: {torch.cuda.get_device_name()}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"  å½“å‰å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n=== åŠ è½½æ•°æ® ===")
    try:
        train_loader, val_loader, test_loader, label_scaler = create_data_loaders(
            config, 
            normalize_labels=True,
            use_descriptors=True,  # å¯ç”¨å¤šæ¨¡æ€
            descriptor_path='descriptor.csv'
        )
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        sample_batch = next(iter(train_loader))
        if len(sample_batch) == 3:
            planes, descriptors, labels = sample_batch
            descriptor_dim = descriptors.shape[1]
            use_multimodal = True
            print(f"âœ“ å¤šæ¨¡æ€æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"  æŠ•å½±å¹³é¢å½¢çŠ¶: {planes.shape}")
            print(f"  æè¿°ç¬¦å½¢çŠ¶: {descriptors.shape}")
            print(f"  æè¿°ç¬¦ç»´åº¦: {descriptor_dim}")
        else:
            planes, labels = sample_batch
            descriptor_dim = 0
            use_multimodal = False
            print(f"âœ“ å•æ¨¡æ€æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"  æŠ•å½±å¹³é¢å½¢çŠ¶: {planes.shape}")
        
        print(f"  æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
        print(f"  æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
        
    except Exception as e:
        print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ¨¡å‹
    print("\n=== åˆ›å»ºæ¨¡å‹ ===")
    try:
        model = create_model(
            config, 
            descriptor_dim=descriptor_dim,
            use_multimodal=use_multimodal
        )
        model = model.to(config.DEVICE)
        
        print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"  æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"  å‚æ•°æ•°é‡: {count_parameters(model):,}")
        print(f"  ä½¿ç”¨å¤šæ¨¡æ€: {use_multimodal}")
        
        # æ˜¾ç¤ºæ¨¡å‹å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            model_memory = torch.cuda.memory_allocated() / 1024**3
            print(f"  æ¨¡å‹å†…å­˜ä½¿ç”¨: {model_memory:.2f} GB")
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\n=== åˆå§‹åŒ–è®­ç»ƒå™¨ ===")
    try:
        trainer = Trainer(model, config, save_dir='optimized_experiments')
        print(f"âœ“ è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"  ä¼˜åŒ–å™¨: {type(trainer.optimizer).__name__}")
        print(f"  å­¦ä¹ ç‡è°ƒåº¦å™¨: {type(trainer.scheduler).__name__}")
        print(f"  æ··åˆç²¾åº¦: {config.USE_MIXED_PRECISION}")
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # å¼€å§‹è®­ç»ƒ
    print("\n=== å¼€å§‹è®­ç»ƒ ===")
    print(f"è®­ç»ƒè½®æ•°: {config.NUM_EPOCHS}")
    print(f"å­¦ä¹ ç‡: {config.LEARNING_RATE}")
    print(f"æƒé‡è¡°å‡: {config.WEIGHT_DECAY}")
    print(f"VAEæƒé‡: {config.VAE_WEIGHT}")
    print(f"VAE Beta: {config.VAE_BETA}")
    
    try:
        # è®­ç»ƒå‰çš„GPUå†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated() / 1024**3
            print(f"\nè®­ç»ƒå‰GPUå†…å­˜: {initial_memory:.2f} GB")
        
        # å¼€å§‹è®­ç»ƒ
        trained_model = trainer.train(train_loader, val_loader, num_epochs=config.NUM_EPOCHS)
        
        print("\n=== è®­ç»ƒå®Œæˆ ===")
        print(f"âœ“ è®­ç»ƒæˆåŠŸå®Œæˆ")
        print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {trainer.train_losses[-1]:.6f}")
        print(f"  æœ€ç»ˆéªŒè¯æŸå¤±: {trainer.val_losses[-1]:.6f}")
        print(f"  æœ€ç»ˆéªŒè¯R2: {trainer.val_r2_scores[-1]:.6f}")
        print(f"  æœ€ä½³éªŒè¯æŸå¤±: {min(trainer.val_losses):.6f}")
        print(f"  æœ€ä½³éªŒè¯R2: {max(trainer.val_r2_scores):.6f}")
        
        # è®­ç»ƒåçš„GPUå†…å­˜çŠ¶æ€
        if torch.cuda.is_available():
            final_memory = torch.cuda.memory_allocated() / 1024**3
            max_memory = torch.cuda.max_memory_allocated() / 1024**3
            print(f"\nGPUå†…å­˜ä½¿ç”¨:")
            print(f"  å½“å‰ä½¿ç”¨: {final_memory:.2f} GB")
            print(f"  å³°å€¼ä½¿ç”¨: {max_memory:.2f} GB")
            print(f"  å†…å­˜æ•ˆç‡: {(max_memory / torch.cuda.get_device_properties(0).total_memory * 1024**3) * 100:.1f}%")
        
        # ä¿å­˜æ¨¡å‹
        model_path = trainer.save_model(experiment_name='optimized_multimodal')
        print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
    except KeyboardInterrupt:
        print("\nâš  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        # ä¿å­˜å½“å‰æ¨¡å‹
        model_path = trainer.save_model(experiment_name='optimized_multimodal_interrupted')
        print(f"âœ“ ä¸­æ–­æ—¶æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
    except Exception as e:
        print(f"\nâœ— è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("\nğŸ‰ ä¼˜åŒ–è®­ç»ƒå®Œæˆï¼")
    print("\næ€§èƒ½ä¼˜åŒ–æ€»ç»“:")
    print(f"  âœ“ æ‰¹æ¬¡å¤§å°å¢åŠ åˆ° {config.BATCH_SIZE}")
    print(f"  âœ“ æ•°æ®åŠ è½½å™¨ä½¿ç”¨ {config.NUM_WORKERS} ä¸ªå·¥ä½œè¿›ç¨‹")
    print(f"  âœ“ å¯ç”¨äº† pin_memory å’Œ persistent_workers")
    print(f"  âœ“ æ··åˆç²¾åº¦è®­ç»ƒ: {config.USE_MIXED_PRECISION}")
    if getattr(config, 'COMPILE_MODEL', False):
        print(f"  âœ“ æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å·²å¯ç”¨")
    if getattr(config, 'CHANNELS_LAST', False):
        print(f"  âœ“ Channels Last å†…å­˜æ ¼å¼ä¼˜åŒ–å·²å¯ç”¨")
    if use_multimodal:
        print(f"  âœ“ å¤šæ¨¡æ€æ¶æ„é›†æˆäº† {descriptor_dim} ç»´æè¿°ç¬¦")

def check_gpu_optimization():
    """æ£€æŸ¥GPUä¼˜åŒ–é…ç½®"""
    print("=== GPUä¼˜åŒ–æ£€æŸ¥ ===")
    
    # PyTorchç‰ˆæœ¬
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # CUDAä¿¡æ¯
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        # æ£€æŸ¥ç¼–è¯‘æ”¯æŒ
        try:
            torch.compile
            print("âœ“ torch.compile æ”¯æŒ")
        except AttributeError:
            print("âœ— torch.compile ä¸æ”¯æŒ (éœ€è¦ PyTorch 2.0+)")
        
        # æ£€æŸ¥æ··åˆç²¾åº¦æ”¯æŒ
        if torch.cuda.is_bf16_supported():
            print("âœ“ BF16 æ··åˆç²¾åº¦æ”¯æŒ")
        else:
            print("âš  BF16 ä¸æ”¯æŒï¼Œä½¿ç”¨ FP16")
        
        # æ£€æŸ¥å†…å­˜æ ¼å¼æ”¯æŒ
        print("âœ“ Channels Last å†…å­˜æ ¼å¼æ”¯æŒ")
        
    else:
        print("âœ— CUDA ä¸å¯ç”¨")
    
    print()

if __name__ == "__main__":
    # æ£€æŸ¥ä¼˜åŒ–é…ç½®
    check_gpu_optimization()
    
    # å¼€å§‹è®­ç»ƒ
    main()