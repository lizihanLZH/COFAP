import os
import sys
import argparse
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import pickle
import h5py
from pathlib import Path
import warnings
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import multiprocessing
warnings.filterwarnings('ignore')
sys.path.append('./BiG-CAE')
sys.path.append('./PP-cVAE')
sys.path.append('./PP-NN')
try:
    from PP-cVAE.models import MultiModalVAECNN, MultiPlaneVAECNN
    from BiG-CAE.contrastive_autoencoder import ContrastiveAutoencoder
    from PP-NN.Net import MultiModalSAGEModel
    from fusion_model import ExactCrossAttentionFusion
    from BiG-CAE.featurizer import get_2cg_inputs_cof
    from PP-NN.ElseFeaturizer import CombinedCOFFeaturizer
    print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸  æ¨¡å—å¯¼å…¥è­¦å‘Š: {e}")
    print("æŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")
class CustomDataPredictor:
    def __init__(self, model_path, device='cuda'):
        if device == 'cuda' and torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_memory < 8.0:  
                print(f"âš ï¸  GPUå†…å­˜è¾ƒå° ({gpu_memory:.1f} GB)ï¼Œå»ºè®®ä½¿ç”¨CPUè¿›è¡Œæ¨ç†")
                print("   å¯ä»¥ä½¿ç”¨ --device cpu å‚æ•°å¼ºåˆ¶ä½¿ç”¨CPU")
                device = 'cpu'
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        if self.device.type == 'cpu':
            print("   ğŸ“ æ³¨æ„ï¼šä½¿ç”¨CPUæ¨ç†ä¼šæ¯”GPUæ…¢ï¼Œä½†å†…å­˜å ç”¨æ›´å°‘")
        print(f"åŠ è½½èåˆæ¨¡å‹: {model_path}")
        self.model = self._load_fusion_model(model_path)
        print("ğŸ”§ æ¨¡å‹åŠ è½½å®Œæˆï¼Œç°åœ¨ç§»åŠ¨åˆ°GPU...")
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            cached_memory = torch.cuda.memory_reserved(0) / 1024**3
            print(f"   ğŸ“Š GPUå†…å­˜ä¿¡æ¯:")
            print(f"      æ€»å†…å­˜: {gpu_memory:.1f} GB")
            print(f"      å·²åˆ†é…: {allocated_memory:.1f} GB")
            print(f"      å·²ç¼“å­˜: {cached_memory:.1f} GB")
            print(f"      å¯ç”¨å†…å­˜: {gpu_memory - cached_memory:.1f} GB")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("   ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜")
        print(f"   ğŸš€ ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡: {self.device}")
        self.model.to(self.device)
        self.model.eval()
        print("âœ… èåˆæ¨¡å‹åŠ è½½æˆåŠŸ")
        self.label_scaler = None
        self._init_label_scaler()
        self._init_descriptor_scaler()
        self._load_configurations()
        print("âœ“ èåˆæ¨¡å‹åŠæ‰€æœ‰å­æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
    def _load_fusion_model(self, model_path):
        print(f"åŠ è½½èåˆæ¨¡å‹: {model_path}")
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("   ğŸ”§ å…ˆåŠ è½½æ¨¡å‹åˆ°CPUï¼Œé¿å…CUDAå†…å­˜ä¸è¶³...")
        checkpoint = torch.load(model_path, map_location='cpu')
        print(f"ğŸ“Š æ£€æŸ¥ç‚¹åŒ…å«çš„é”®: {list(checkpoint.keys()) if isinstance(checkpoint, dict) else 'ç›´æ¥æ˜¯state_dict'}")
        descriptor_dim = 12  
        print("   ğŸ”§ åœ¨CPUä¸Šåˆ›å»ºæ¨¡å‹å®ä¾‹...")
        model = ExactCrossAttentionFusion(
            vae_config=checkpoint.get('vae_config', {}),
            use_descriptors=True,  
            descriptor_dim=descriptor_dim,  
            fusion_dim=128
        )
        print("   ğŸ”§ åœ¨CPUä¸ŠåŠ è½½æ¨¡å‹æƒé‡...")
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print("âœ… ä»model_state_dictåŠ è½½æƒé‡")
        else:
            model.load_state_dict(checkpoint)
            print("âœ… ç›´æ¥åŠ è½½æƒé‡")
        print("ğŸ” æ£€æŸ¥èåˆæ¨¡å‹checkpointä¸­çš„å­æ¨¡å‹æƒé‡...")
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        vae_keys = [key for key in state_dict.keys() if key.startswith('vae_model')]
        gcn_cc_keys = [key for key in state_dict.keys() if key.startswith('gcn_cc_model')]
        gcn_noncc_keys = [key for key in state_dict.keys() if key.startswith('gcn_noncc_model')]
        lzhnn_keys = [key for key in state_dict.keys() if key.startswith('lzhnn_model')]
        print(f"   ğŸ“Š æƒé‡é”®åˆ†æ:")
        print(f"     VAEæ¨¡å‹æƒé‡: {len(vae_keys)} ä¸ª")
        print(f"     GCN CCæ¨¡å‹æƒé‡: {len(gcn_cc_keys)} ä¸ª")
        print(f"     GCN Non-CCæ¨¡å‹æƒé‡: {len(gcn_noncc_keys)} ä¸ª")
        print(f"     LZHNNæ¨¡å‹æƒé‡: {len(lzhnn_keys)} ä¸ª")
        if vae_keys and gcn_cc_keys and gcn_noncc_keys and lzhnn_keys:
            print("   âœ… æ£€æµ‹åˆ°å®Œæ•´çš„å­æ¨¡å‹æƒé‡ï¼Œèåˆæ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")
            print("   ğŸ“ æ³¨æ„ï¼šå­æ¨¡å‹æƒé‡å·²åŒ…å«åœ¨èåˆæ¨¡å‹checkpointä¸­")
        else:
            print("   âš ï¸  æœªæ£€æµ‹åˆ°å®Œæ•´çš„å­æ¨¡å‹æƒé‡")
            print("   ğŸ“ æ³¨æ„ï¼šè¿™å¯èƒ½æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´checkpointï¼Œå­æ¨¡å‹æƒé‡å¯èƒ½ä¸å®Œæ•´")
            if vae_keys:
                print("   ğŸ” å°è¯•ä»checkpointä¸­æå–VAEæƒé‡...")
                try:
                    vae_state_dict = {key[11:]: value for key, value in state_dict.items() if key.startswith('vae_model.')}
                    if vae_state_dict:
                        missing_keys, unexpected_keys = model.vae_model.load_state_dict(vae_state_dict, strict=False)
                        if missing_keys:
                            print(f"     âš ï¸  VAEç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
                        if unexpected_keys:
                            print(f"     âš ï¸  VAEæ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
                        print("     âœ… VAEæƒé‡æå–æˆåŠŸ")
                    else:
                        print("     âŒ æ— æ³•æå–VAEæƒé‡")
                except Exception as e:
                    print(f"     âŒ VAEæƒé‡æå–å¤±è´¥: {e}")
            if gcn_cc_keys:
                print("   ğŸ” å°è¯•ä»checkpointä¸­æå–GCN CCæƒé‡...")
                try:
                    gcn_cc_state_dict = {key[15:]: value for key, value in state_dict.items() if key.startswith('gcn_cc_model.')}
                    if gcn_cc_state_dict:
                        missing_keys, unexpected_keys = model.gcn_cc_model.load_state_dict(gcn_cc_state_dict, strict=False)
                        if missing_keys:
                            print(f"     âš ï¸  GCN CCç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
                        if unexpected_keys:
                            print(f"     âš ï¸  GCN CCæ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
                        print("     âœ… GCN CCæƒé‡æå–æˆåŠŸ")
                    else:
                        print("     âŒ æ— æ³•æå–GCN CCæƒé‡")
                except Exception as e:
                    print(f"     âŒ GCN CCæƒé‡æå–å¤±è´¥: {e}")
            if gcn_noncc_keys:
                print("   ğŸ” å°è¯•ä»checkpointä¸­æå–GCN Non-CCæƒé‡...")
                try:
                    gcn_noncc_state_dict = {key[18:]: value for key, value in state_dict.items() if key.startswith('gcn_noncc_model.')}
                    if gcn_noncc_state_dict:
                        missing_keys, unexpected_keys = model.gcn_noncc_model.load_state_dict(gcn_noncc_state_dict, strict=False)
                        if missing_keys:
                            print(f"     âš ï¸  GCN Non-CCç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
                        if unexpected_keys:
                            print(f"     âš ï¸  GCN Non-CCæ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
                        print("     âœ… GCN Non-CCæƒé‡æå–æˆåŠŸ")
                    else:
                        print("     âŒ æ— æ³•æå–GCN Non-CCæƒé‡")
                except Exception as e:
                    print(f"     âŒ GCN Non-CCæƒé‡æå–å¤±è´¥: {e}")
            if lzhnn_keys:
                print("   ğŸ” å°è¯•ä»checkpointä¸­æå–LZHNNæƒé‡...")
                try:
                    lzhnn_state_dict = {key[13:]: value for key, value in state_dict.items() if key.startswith('lzhnn_model.')}
                    if lzhnn_state_dict:
                        missing_keys, unexpected_keys = model.lzhnn_model.load_state_dict(lzhnn_state_dict, strict=False)
                        if missing_keys:
                            print(f"     âš ï¸  LZHNNç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
                        if unexpected_keys:
                            print(f"     âš ï¸  LZHNNæ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
                        print("     âœ… LZHNNæƒé‡æå–æˆåŠŸ")
                    else:
                        print("     âŒ æ— æ³•æå–LZHNNæƒé‡")
                except Exception as e:
                    print(f"     âŒ LZHNNæƒé‡æå–å¤±è´¥: {e}")
        print("   ğŸ”§ åœ¨CPUä¸ŠéªŒè¯æ¨¡å‹æƒé‡...")
        self._verify_model_weights(model)
        print("âœ… èåˆæ¨¡å‹åŠ è½½æˆåŠŸ")
        print("   ğŸ”§ æ¨¡å‹ç°åœ¨åœ¨CPUä¸Šï¼Œç¨åå°†ç§»åŠ¨åˆ°GPU")
        return model
    def _verify_model_weights(self, model):
        print("ğŸ” éªŒè¯æ¨¡å‹æƒé‡...")
        if hasattr(model, 'vae_model') and model.vae_model is not None:
            try:
                if hasattr(model.vae_model, 'vae') and hasattr(model.vae_model.vae, 'encoder'):
                    vae_conv1_weight = model.vae_model.vae.encoder[0].weight.data
                    vae_conv1_mean = vae_conv1_weight.mean().item()
                    vae_conv1_std = vae_conv1_weight.std().item()
                    print(f"   VAEç¬¬ä¸€å±‚å·ç§¯æƒé‡: å‡å€¼={vae_conv1_mean:.6f}, æ ‡å‡†å·®={vae_conv1_std:.6f}")
                    if abs(vae_conv1_mean) < 0.01 and vae_conv1_std < 0.1:
                        print("   âš ï¸  è­¦å‘Š: VAEæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                    else:
                        print("   âœ… VAEæƒé‡åŠ è½½æ­£å¸¸")
                elif hasattr(model.vae_model, 'encoder'):
                    vae_conv1_weight = model.vae_model.encoder[0].weight.data
                    vae_conv1_mean = vae_conv1_weight.mean().item()
                    vae_conv1_std = vae_conv1_weight.std().item()
                    print(f"   VAEç¬¬ä¸€å±‚å·ç§¯æƒé‡: å‡å€¼={vae_conv1_mean:.6f}, æ ‡å‡†å·®={vae_conv1_std:.6f}")
                    if abs(vae_conv1_mean) < 0.01 and vae_conv1_std < 0.1:
                        print("   âš ï¸  è­¦å‘Š: VAEæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                    else:
                        print("   âœ… VAEæƒé‡åŠ è½½æ­£å¸¸")
                else:
                    print("   âš ï¸  æ— æ³•è¯†åˆ«VAEæ¨¡å‹ç»“æ„")
            except Exception as e:
                print(f"   âš ï¸  æ£€æŸ¥VAEæƒé‡æ—¶å‡ºé”™: {e}")
        if hasattr(model, 'gcn_cc_model') and model.gcn_cc_model is not None:
            try:
                print(f"   ğŸ” æ£€æŸ¥GCN CCæ¨¡å‹æƒé‡...")
                if hasattr(model.gcn_cc_model, 'encoder'):
                    if hasattr(model.gcn_cc_model.encoder, 'conv_layers') and len(model.gcn_cc_model.encoder.conv_layers) > 0:
                        first_conv = model.gcn_cc_model.encoder.conv_layers[0]
                        if hasattr(first_conv, 'weight'):
                            for edge_type, conv_layer in first_conv.mods.items():
                                if hasattr(conv_layer, 'weight'):
                                    gcn_weight = conv_layer.weight.data
                                    gcn_mean = gcn_weight.mean().item()
                                    gcn_std = gcn_weight.std().item()
                                    print(f"   GCN CCç¼–ç å™¨ç¬¬ä¸€å±‚å·ç§¯({edge_type})æƒé‡: å‡å€¼={gcn_mean:.6f}, æ ‡å‡†å·®={gcn_std:.6f}")
                                    if abs(gcn_mean) < 0.01 and gcn_std < 0.1:
                                        print(f"      âš ï¸  è­¦å‘Š: GCN CC {edge_type}æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                                    else:
                                        print(f"      âœ… GCN CC {edge_type}æƒé‡åŠ è½½æ­£å¸¸")
                        else:
                            print("   âš ï¸  æ— æ³•è¯†åˆ«GCN CCå·ç§¯å±‚ç»“æ„")
                        if hasattr(model.gcn_cc_model.encoder, 'latent_mapping'):
                            latent_first_layer = model.gcn_cc_model.encoder.latent_mapping[0]
                            if hasattr(latent_first_layer, 'weight'):
                                latent_weight = latent_first_layer.weight.data
                                latent_mean = latent_weight.mean().item()
                                latent_std = latent_weight.std().item()
                                print(f"   GCN CCç¼–ç å™¨æ½œåœ¨æ˜ å°„å±‚æƒé‡: å‡å€¼={latent_mean:.6f}, æ ‡å‡†å·®={latent_std:.6f}")
                                if abs(latent_mean) < 0.01 and latent_std < 0.1:
                                    print("      âš ï¸  è­¦å‘Š: GCN CCæ½œåœ¨æ˜ å°„æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                                else:
                                    print("      âœ… GCN CCæ½œåœ¨æ˜ å°„æƒé‡åŠ è½½æ­£å¸¸")
                    else:
                        print("   âš ï¸  æ— æ³•è¯†åˆ«GCN CCç¼–ç å™¨ç»“æ„")
                else:
                    print("   âš ï¸  æ— æ³•è¯†åˆ«GCN CCæ¨¡å‹ç»“æ„")
            except Exception as e:
                print(f"   âš ï¸  æ£€æŸ¥GCN CCæƒé‡æ—¶å‡ºé”™: {e}")
        if hasattr(model, 'gcn_noncc_model') and model.gcn_noncc_model is not None:
            try:
                print(f"   ğŸ” æ£€æŸ¥GCN Non-CCæ¨¡å‹æƒé‡...")
                if hasattr(model.gcn_noncc_model, 'encoder'):
                    if hasattr(model.gcn_noncc_model.encoder, 'conv_layers') and len(model.gcn_noncc_model.encoder.conv_layers) > 0:
                        first_conv = model.gcn_noncc_model.encoder.conv_layers[0]
                        if hasattr(first_conv, 'mods'):
                            for edge_type, conv_layer in first_conv.mods.items():
                                if hasattr(conv_layer, 'weight'):
                                    gcn_weight = conv_layer.weight.data
                                    gcn_mean = gcn_weight.mean().item()
                                    gcn_std = gcn_weight.std().item()
                                    print(f"   GCN Non-CCç¼–ç å™¨ç¬¬ä¸€å±‚å·ç§¯({edge_type})æƒé‡: å‡å€¼={gcn_mean:.6f}, æ ‡å‡†å·®={gcn_std:.6f}")
                                    if abs(gcn_mean) < 0.01 and gcn_std < 0.1:
                                        print(f"      âš ï¸  è­¦å‘Š: GCN Non-CC {edge_type}æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                                    else:
                                        print(f"      âœ… GCN Non-CC {edge_type}æƒé‡åŠ è½½æ­£å¸¸")
                        else:
                            print("   âš ï¸  æ— æ³•è¯†åˆ«GCN Non-CCå·ç§¯å±‚ç»“æ„")
                        if hasattr(model.gcn_noncc_model.encoder, 'latent_mapping'):
                            latent_first_layer = model.gcn_noncc_model.encoder.latent_mapping[0]
                            if hasattr(latent_first_layer, 'weight'):
                                latent_weight = latent_first_layer.weight.data
                                latent_mean = latent_weight.mean().item()
                                latent_std = latent_weight.std().item()
                                print(f"   GCN Non-CCç¼–ç å™¨æ½œåœ¨æ˜ å°„å±‚æƒé‡: å‡å€¼={latent_mean:.6f}, æ ‡å‡†å·®={latent_std:.6f}")
                                if abs(latent_mean) < 0.01 and latent_std < 0.1:
                                    print("      âš ï¸  è­¦å‘Š: GCN Non-CCæ½œåœ¨æ˜ å°„æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                                else:
                                    print("      âœ… GCN Non-CCæ½œåœ¨æ˜ å°„æƒé‡åŠ è½½æ­£å¸¸")
                    else:
                        print("   âš ï¸  æ— æ³•è¯†åˆ«GCN Non-CCç¼–ç å™¨ç»“æ„")
                else:
                    print("   âš ï¸  æ— æ³•è¯†åˆ«GCN Non-CCæ¨¡å‹ç»“æ„")
            except Exception as e:
                print(f"   âš ï¸  æ£€æŸ¥GCN Non-CCæƒé‡æ—¶å‡ºé”™: {e}")
        if hasattr(model, 'gcn_model') and model.gcn_model is not None:
            try:
                print(f"   ğŸ” æ£€æŸ¥æ—§ç‰ˆGCNæ¨¡å‹æƒé‡...")
                if hasattr(model.gcn_model, 'conv1'):
                    gcn_conv1_weight = model.gcn_model.conv1.weight.data
                    gcn_conv1_mean = gcn_conv1_weight.mean().item()
                    gcn_conv1_std = gcn_conv1_weight.std().item()
                    print(f"   GCNç¬¬ä¸€å±‚æƒé‡: å‡å€¼={gcn_conv1_mean:.6f}, æ ‡å‡†å·®={gcn_conv1_std:.6f}")
                    if abs(gcn_conv1_mean) < 0.01 and gcn_conv1_std < 0.1:
                        print("   âš ï¸  è­¦å‘Š: GCNæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                    else:
                        print("   âœ… GCNæƒé‡åŠ è½½æ­£å¸¸")
                elif hasattr(model.gcn_model, 'layers') and len(model.gcn_model.layers) > 0:
                    first_layer = model.gcn_model.layers[0]
                    if hasattr(first_layer, 'weight'):
                        gcn_weight = first_layer.weight.data
                        gcn_mean = gcn_weight.mean().item()
                        gcn_std = gcn_weight.std().item()
                        print(f"   GCNç¬¬ä¸€å±‚æƒé‡: å‡å€¼={gcn_mean:.6f}, æ ‡å‡†å·®={gcn_std:.6f}")
                        if abs(gcn_mean) < 0.01 and gcn_std < 0.1:
                            print("   âš ï¸  è­¦å‘Š: GCNæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                        else:
                            print("   âœ… GCNæƒé‡åŠ è½½æ­£å¸¸")
                    else:
                        print("   âš ï¸  æ— æ³•è¯†åˆ«GCNæ¨¡å‹ç»“æ„")
                else:
                    print("   âš ï¸  æ— æ³•è¯†åˆ«GCNæ¨¡å‹ç»“æ„")
            except Exception as e:
                print(f"   âš ï¸  æ£€æŸ¥GCNæƒé‡æ—¶å‡ºé”™: {e}")
        if hasattr(model, 'lzhnn_model') and model.lzhnn_model is not None:
            try:
                if hasattr(model.lzhnn_model, 'topo_mlp'):
                    topo_first_layer = model.lzhnn_model.topo_mlp[0]  
                    if hasattr(topo_first_layer, 'weight'):
                        topo_weight = topo_first_layer.weight.data
                        topo_mean = topo_weight.mean().item()
                        topo_std = topo_weight.std().item()
                        print(f"   LZHNNæ‹“æ‰‘MLPç¬¬ä¸€å±‚æƒé‡: å‡å€¼={topo_mean:.6f}, æ ‡å‡†å·®={topo_std:.6f}")
                        if abs(topo_mean) < 0.001 and topo_std < 0.01:
                            print("   âš ï¸  è­¦å‘Š: LZHNNæ‹“æ‰‘MLPæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                        else:
                            print("   âœ… LZHNNæ‹“æ‰‘MLPæƒé‡åŠ è½½æ­£å¸¸")
                    if hasattr(model.lzhnn_model, 'struct_mlp'):
                        struct_first_layer = model.lzhnn_model.struct_mlp[0]
                        if hasattr(struct_first_layer, 'weight'):
                            struct_weight = struct_first_layer.weight.data
                            struct_mean = struct_weight.mean().item()
                            struct_std = struct_weight.std().item()
                            print(f"   LZHNNç»“æ„MLPç¬¬ä¸€å±‚æƒé‡: å‡å€¼={struct_mean:.6f}, æ ‡å‡†å·®={struct_std:.6f}")
                            if abs(struct_mean) < 0.001 and struct_std < 0.01:
                                print("   âš ï¸  è­¦å‘Š: LZHNNç»“æ„MLPæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                            else:
                                print("   âœ… LZHNNç»“æ„MLPæƒé‡åŠ è½½æ­£å¸¸")
                    print("   âœ… LZHNNæ¨¡å‹ç»“æ„è¯†åˆ«æˆåŠŸ")
                elif hasattr(model.lzhnn_model, 'fc1'):
                    lzhnn_fc1_weight = model.lzhnn_model.fc1.weight.data
                    lzhnn_fc1_mean = lzhnn_fc1_weight.mean().item()
                    lzhnn_fc1_std = lzhnn_fc1_weight.std().item()
                    print(f"   LZHNNç¬¬ä¸€å±‚æƒé‡: å‡å€¼={lzhnn_fc1_mean:.6f}, æ ‡å‡†å·®={lzhnn_fc1_std:.6f}")
                    if abs(lzhnn_fc1_mean) < 0.001 and lzhnn_fc1_std < 0.01:
                        print("   âš ï¸  è­¦å‘Š: LZHNNæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                    else:
                        print("   âœ… LZHNNæƒé‡åŠ è½½æ­£å¸¸")
                elif hasattr(model.lzhnn_model, 'layers') and len(model.lzhnn_model.layers) > 0:
                    first_layer = model.lzhnn_model.layers[0]
                    if hasattr(first_layer, 'weight'):
                        lzhnn_weight = first_layer.weight.data
                        lzhnn_mean = lzhnn_weight.mean().item()
                        lzhnn_std = lzhnn_weight.std().item()
                        print(f"   LZHNNç¬¬ä¸€å±‚æƒé‡: å‡å€¼={lzhnn_mean:.6f}, æ ‡å‡†å·®={lzhnn_std:.6f}")
                        if abs(lzhnn_mean) < 0.001 and lzhnn_std < 0.01:
                            print("   âš ï¸  è­¦å‘Š: LZHNNæƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                        else:
                            print("   âœ… LZHNNæƒé‡åŠ è½½æ­£å¸¸")
                    else:
                        print("   âš ï¸  æ— æ³•è¯†åˆ«LZHNNæ¨¡å‹ç»“æ„")
                else:
                    print("   âš ï¸  æ— æ³•è¯†åˆ«LZHNNæ¨¡å‹ç»“æ„")
            except Exception as e:
                print(f"   âš ï¸  æ£€æŸ¥LZHNNæƒé‡æ—¶å‡ºé”™: {e}")
        if hasattr(model, 'fusion_layer'):
            try:
                fusion_weight = model.fusion_layer.weight.data
                fusion_mean = fusion_weight.mean().item()
                fusion_std = fusion_weight.std().item()
                print(f"   èåˆå±‚æƒé‡: å‡å€¼={fusion_mean:.6f}, æ ‡å‡†å·®={fusion_std:.6f}")
                if abs(fusion_mean) < 0.01 and fusion_std < 0.1:
                    print("   âš ï¸  è­¦å‘Š: èåˆå±‚æƒé‡å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼ˆæ¥è¿‘åˆå§‹åŒ–å€¼ï¼‰")
                else:
                    print("   âœ… èåˆå±‚æƒé‡åŠ è½½æ­£å¸¸")
            except Exception as e:
                print(f"   âš ï¸  æ£€æŸ¥èåˆå±‚æƒé‡æ—¶å‡ºé”™: {e}")
        print("   ğŸ“Š æ¨¡å‹ç»“æ„ä¿¡æ¯:")
        print(f"     æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"     æ¨¡å‹å±æ€§: {[attr for attr in dir(model) if not attr.startswith('_')]}")
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"     æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"     å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    def _load_configurations(self):
        print("åŠ è½½é…ç½®å’Œæ•°æ®...")
        self._verify_training_inference_consistency()
        print("âœ… é…ç½®åŠ è½½å®Œæˆ")
    def _verify_training_inference_consistency(self):
        print("ğŸ” éªŒè¯è®­ç»ƒå’Œæ¨ç†æ•°æ®ä¸€è‡´æ€§...")
        train_labels_path = "labels_CH410.csv"
        if os.path.exists(train_labels_path):
            train_labels_size = os.path.getsize(train_labels_path)
            train_labels_mtime = os.path.getmtime(train_labels_path)
            print(f"   âœ… è®­ç»ƒæ ‡ç­¾æ–‡ä»¶: {train_labels_path}")
            print(f"      å¤§å°: {train_labels_size} bytes")
            print(f"      ä¿®æ”¹æ—¶é—´: {pd.Timestamp.fromtimestamp(train_labels_mtime)}")
        else:
            print(f"   âŒ è®­ç»ƒæ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {train_labels_path}")
        train_descriptor_path = "VAE/descriptor.csv"
        if os.path.exists(train_descriptor_path):
            train_descriptor_size = os.path.getsize(train_descriptor_path)
            train_descriptor_mtime = os.path.getmtime(train_descriptor_path)
            print(f"   âœ… è®­ç»ƒæè¿°ç¬¦æ–‡ä»¶: {train_descriptor_path}")
            print(f"      å¤§å°: {train_descriptor_size} bytes")
            print(f"      ä¿®æ”¹æ—¶é—´: {pd.Timestamp.fromtimestamp(train_descriptor_mtime)}")
        else:
            print(f"   âŒ è®­ç»ƒæè¿°ç¬¦æ–‡ä»¶ä¸å­˜åœ¨: {train_descriptor_path}")
        train_linkers_path = "GCN/linkers.csv"
        if os.path.exists(train_linkers_path):
            train_linkers_size = os.path.getsize(train_linkers_path)
            train_linkers_mtime = os.path.getmtime(train_linkers_path)
            print(f"   âœ… è®­ç»ƒé“¾æ¥å™¨æ–‡ä»¶: {train_linkers_path}")
            print(f"      å¤§å°: {train_linkers_size} bytes")
            print(f"      ä¿®æ”¹æ—¶é—´: {pd.Timestamp.fromtimestamp(train_linkers_mtime)}")
        else:
            print(f"   âŒ è®­ç»ƒé“¾æ¥å™¨æ–‡ä»¶ä¸å­˜åœ¨: {train_linkers_path}")
        self._verify_scaler_consistency()
        print("   ğŸ“Š æ•°æ®ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
    def _verify_scaler_consistency(self):
        print("   ğŸ” éªŒè¯æ ‡å‡†åŒ–å™¨ä¸€è‡´æ€§...")
        if hasattr(self, 'train_label_stats') and self.train_label_stats is not None:
            current_mean = self.label_scaler.mean_[0] if hasattr(self.label_scaler, 'mean_') else None
            current_scale = self.label_scaler.scale_[0] if hasattr(self.label_scaler, 'scale_') else None
            if current_mean is not None and current_scale is not None:
                mean_diff = abs(current_mean - self.train_label_stats['mean'][0])
                scale_diff = abs(current_scale - self.train_label_stats['scale'][0])
                print(f"     æ ‡ç­¾æ ‡å‡†åŒ–å™¨:")
                print(f"       è®­ç»ƒæ—¶å‡å€¼: {self.train_label_stats['mean'][0]:.6f}")
                print(f"       å½“å‰å‡å€¼: {current_mean:.6f}")
                print(f"       å‡å€¼å·®å¼‚: {mean_diff:.8f}")
                print(f"       è®­ç»ƒæ—¶æ ‡å‡†å·®: {self.train_label_stats['scale'][0]:.6f}")
                print(f"       å½“å‰æ ‡å‡†å·®: {current_scale:.6f}")
                print(f"       æ ‡å‡†å·®å·®å¼‚: {scale_diff:.8f}")
                if mean_diff > 1e-6 or scale_diff > 1e-6:
                    print(f"       âš ï¸  è­¦å‘Š: æ ‡ç­¾æ ‡å‡†åŒ–å™¨å‚æ•°ä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´")
                else:
                    print(f"       âœ… æ ‡ç­¾æ ‡å‡†åŒ–å™¨å‚æ•°ä¸€è‡´")
            else:
                print(f"      âš ï¸  æ— æ³•è·å–å½“å‰æ ‡ç­¾æ ‡å‡†åŒ–å™¨å‚æ•°")
        else:
            print(f"      âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ—¶æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯")
        if hasattr(self, 'train_descriptor_stats') and self.train_descriptor_stats is not None:
            current_mean = self.descriptor_scaler.mean_ if hasattr(self.descriptor_scaler, 'mean_') else None
            current_scale = self.descriptor_scaler.scale_ if hasattr(self.descriptor_scaler, 'scale_') else None
            if current_mean is not None and current_scale is not None:
                mean_diff = np.mean(np.abs(current_mean - self.train_descriptor_stats['mean']))
                scale_diff = np.mean(np.abs(current_scale - self.train_descriptor_stats['scale']))
                print(f"     æè¿°ç¬¦æ ‡å‡†åŒ–å™¨:")
                print(f"       è®­ç»ƒæ—¶å‡å€¼èŒƒå›´: {self.train_descriptor_stats['mean'].min():.6f} - {self.train_descriptor_stats['mean'].max():.6f}")
                print(f"       å½“å‰å‡å€¼èŒƒå›´: {current_mean.min():.6f} - {current_mean.max():.6f}")
                print(f"       å¹³å‡å‡å€¼å·®å¼‚: {mean_diff:.8f}")
                print(f"       è®­ç»ƒæ—¶æ ‡å‡†å·®èŒƒå›´: {self.train_descriptor_stats['scale'].min():.6f} - {self.train_descriptor_stats['scale'].max():.6f}")
                print(f"       å½“å‰æ ‡å‡†å·®èŒƒå›´: {current_scale.min():.6f} - {current_scale.max():.6f}")
                print(f"       å¹³å‡æ ‡å‡†å·®å·®å¼‚: {scale_diff:.8f}")
                if mean_diff > 1e-6 or scale_diff > 1e-6:
                    print(f"       âš ï¸  è­¦å‘Š: æè¿°ç¬¦æ ‡å‡†åŒ–å™¨å‚æ•°ä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´")
                else:
                    print(f"       âœ… æè¿°ç¬¦æ ‡å‡†åŒ–å™¨å‚æ•°ä¸€è‡´")
            else:
                print(f"      âš ï¸  æ— æ³•è·å–å½“å‰æè¿°ç¬¦æ ‡å‡†åŒ–å™¨å‚æ•°")
        else:
            print(f"      âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ—¶æè¿°ç¬¦ç»Ÿè®¡ä¿¡æ¯")
    def _custom_collate_fn(self, batch):
        sample = batch[0]
        for key, value in sample.items():
            if value is None:
                if key in ['graph_cc', 'graph_noncc']:
                    import dgl
                    sample[key] = dgl.heterograph({
                        ("l", "l2n", "n"): (torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)),
                        ("n", "n2l", "l"): (torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)),
                    })
                    sample[key].nodes["l"].data["feat"] = torch.zeros(0, 300, dtype=torch.float32)
                    sample[key].nodes["n"].data["feat"] = torch.zeros(0, 4, dtype=torch.float32)
                elif key == 'lzhnn_data':
                    sample[key] = {
                        'topo': torch.zeros(18, dtype=torch.float32),
                        'chem': torch.zeros(300, dtype=torch.float32),
                        'struct': torch.zeros(5, dtype=torch.float32)
                    }
        return sample
    def prepare_new_data(self, data_config, use_parallel=True, num_workers=None):
        print("å‡†å¤‡æ–°æ•°æ®...")
        self._verify_data_config_consistency(data_config)
        dataset = CustomDataset(
            data_config, 
            "GCN/linkers.csv", 
            self.descriptor_scaler, 
            self.descriptor_cols,
            use_parallel=use_parallel,
            num_workers=num_workers
        )
        dataloader = DataLoader(
            dataset, 
            batch_size=1, 
            shuffle=False, 
            num_workers=0,
            collate_fn=self._custom_collate_fn
        )  
        return dataloader
    def _verify_data_config_consistency(self, data_config):
        print("ğŸ” éªŒè¯æ•°æ®é…ç½®ä¸€è‡´æ€§...")
        if hasattr(self, 'descriptor_cols') and self.descriptor_cols is not None:
            try:
                pred_descriptor_df = pd.read_csv(data_config['descriptor_csv'])
                pred_cols = [col for col in pred_descriptor_df.columns if col != 'name']
                print(f"   ğŸ“Š æè¿°ç¬¦åˆ—ä¸€è‡´æ€§æ£€æŸ¥:")
                print(f"     è®­ç»ƒæ—¶åˆ—æ•°: {len(self.descriptor_cols)}")
                print(f"     é¢„æµ‹æ—¶åˆ—æ•°: {len(pred_cols)}")
                train_cols_set = set(self.descriptor_cols)
                pred_cols_set = set(pred_cols)
                missing_in_pred = train_cols_set - pred_cols_set
                extra_in_pred = pred_cols_set - train_cols_set
                if missing_in_pred:
                    print(f"     âš ï¸  é¢„æµ‹æ—¶ç¼ºå¤±åˆ—: {list(missing_in_pred)}")
                if extra_in_pred:
                    print(f"     âš ï¸  é¢„æµ‹æ—¶å¤šä½™åˆ—: {list(extra_in_pred)}")
                if not missing_in_pred and not extra_in_pred:
                    print(f"     âœ… æè¿°ç¬¦åˆ—å®Œå…¨ä¸€è‡´")
                elif len(missing_in_pred) <= 2:  
                    print(f"     âš ï¸  æè¿°ç¬¦åˆ—åŸºæœ¬ä¸€è‡´ï¼Œå°‘é‡ç¼ºå¤±")
                else:
                    print(f"     âŒ æè¿°ç¬¦åˆ—å·®å¼‚è¾ƒå¤§")
            except Exception as e:
                print(f"     âš ï¸  æ— æ³•éªŒè¯æè¿°ç¬¦åˆ—ä¸€è‡´æ€§: {e}")
        print(f"   ğŸ“ æ•°æ®ç›®å½•é…ç½®:")
        for key, path in data_config.items():
            if os.path.exists(path):
                if os.path.isfile(path):
                    size = os.path.getsize(path)
                    print(f"     {key}: {path} ({size} bytes) âœ…")
                else:
                    file_count = len(list(Path(path).glob('*')))
                    print(f"     {key}: {path} ({file_count} æ–‡ä»¶) âœ…")
            else:
                print(f"     {key}: {path} (ä¸å­˜åœ¨) âŒ")
        print("   ğŸ“Š æ•°æ®é…ç½®ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
    def predict(self, dataloader, output_file=None):
        print("ğŸš€ å¼€å§‹é¢„æµ‹...")
        self._verify_prediction_data_consistency()
        all_predictions = []
        all_identifiers = []
        total_batches = len(dataloader)
        progress_interval = max(1, total_batches // 100)
        data_quality_stats = {
            'vae_data_zeros': 0,
            'gcn_data_empty': 0,
            'lzhnn_data_zeros': 0,
            'descriptors_zeros': 0
        }
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx % progress_interval == 0 or batch_idx == total_batches - 1:
                    progress = (batch_idx + 1) / total_batches * 100
                    print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({progress:.1f}%)")
                else:
                    print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}", end="\r")
                self._check_batch_data_quality(batch, data_quality_stats)
                if batch_idx % progress_interval == 0 or batch_idx == total_batches - 1:
                    print("    ğŸ”§ å‡†å¤‡è¾“å…¥æ•°æ®...")
                inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
                if batch_idx % progress_interval == 0 or batch_idx == total_batches - 1:
                    print("    ğŸ§  è¿›è¡Œæ¨¡å‹é¢„æµ‹...")
                outputs = self.model(inputs)
                if isinstance(outputs, torch.Tensor):
                    batch_predictions = outputs.cpu().numpy().flatten()
                else:
                    batch_predictions = outputs['prediction'].cpu().numpy().flatten()
                all_predictions.extend(batch_predictions)
                if 'identifiers' in batch:
                    all_identifiers.extend(batch['identifiers'])
                else:
                    all_identifiers.extend([f"sample_{batch_idx}_{i}" for i in range(len(batch_predictions))])
                if batch_idx % progress_interval == 0 or batch_idx == total_batches - 1:
                    batch_predictions_array = np.array(batch_predictions)
                    batch_lg_predictions = self._denormalize_predictions(batch_predictions_array)
                    print(f"    âœ… æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ")
                    print(f"       æ ‡å‡†åŒ–é¢„æµ‹å€¼: {batch_predictions}")
                    print(f"       lgé¢„æµ‹å€¼: {batch_lg_predictions}")
        print("\nğŸ“Š æ•°æ®è´¨é‡ç»Ÿè®¡:")
        print(f"   VAEæ•°æ®å…¨é›¶æ ·æœ¬: {data_quality_stats['vae_data_zeros']}")
        print(f"   GCNæ•°æ®ç©ºå›¾æ ·æœ¬: {data_quality_stats['gcn_data_empty']}")
        print(f"   LZHNNæ•°æ®å…¨é›¶æ ·æœ¬: {data_quality_stats['lzhnn_data_zeros']}")
        print(f"   æè¿°ç¬¦å…¨é›¶æ ·æœ¬: {data_quality_stats['descriptors_zeros']}")
        print("\nğŸ”„ åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ...")
        all_predictions_array = np.array(all_predictions)
        lg_predictions = self._denormalize_predictions(all_predictions_array)
        print(f"\nğŸ‰ é¢„æµ‹å®Œæˆï¼Œå…±å¤„ç† {len(all_predictions)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“Š é¢„æµ‹å€¼ç»Ÿè®¡:")
        print(f"   æ ‡å‡†åŒ–é¢„æµ‹å€¼èŒƒå›´: {all_predictions_array.min():.4f} - {all_predictions_array.max():.4f}")
        print(f"   æ ‡å‡†åŒ–é¢„æµ‹å€¼å‡å€¼: {np.mean(all_predictions_array):.4f}")
        print(f"   æ ‡å‡†åŒ–é¢„æµ‹å€¼æ ‡å‡†å·®: {np.std(all_predictions_array):.4f}")
        print(f"   lgé¢„æµ‹å€¼èŒƒå›´: {lg_predictions.min():.4f} - {lg_predictions.max():.4f}")
        print(f"   lgé¢„æµ‹å€¼å‡å€¼: {np.mean(lg_predictions):.4f}")
        self._verify_training_prediction_consistency(all_identifiers, all_predictions_array, lg_predictions)
        results_df = pd.DataFrame({
            'identifier': all_identifiers,
            'prediction_normalized': all_predictions_array,  
            'prediction_lg': lg_predictions                  
        })
        if output_file:
            results_df.to_csv(output_file, index=False)
            print(f"ğŸ“ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return results_df
    def _verify_prediction_data_consistency(self):
        print("ğŸ” éªŒè¯é¢„æµ‹è¿‡ç¨‹ä¸­æ•°æ®ä¸€è‡´æ€§...")
        if hasattr(self, 'train_label_stats') and self.train_label_stats is not None:
            current_mean = self.label_scaler.mean_[0] if hasattr(self.label_scaler, 'mean_') else None
            current_scale = self.label_scaler.scale_[0] if hasattr(self.label_scaler, 'scale_') else None
            if current_mean is not None and current_scale is not None:
                mean_diff = abs(current_mean - self.train_label_stats['mean'][0])
                scale_diff = abs(current_scale - self.train_label_stats['scale'][0])
                if mean_diff > 1e-6 or scale_diff > 1e-6:
                    print(f"   âš ï¸  è­¦å‘Š: æ ‡ç­¾æ ‡å‡†åŒ–å™¨å‚æ•°åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿå˜åŒ–")
                    print(f"     è®­ç»ƒæ—¶å‡å€¼: {self.train_label_stats['mean'][0]:.6f}")
                    print(f"     å½“å‰å‡å€¼: {current_mean:.6f}")
                    print(f"     å‡å€¼å·®å¼‚: {mean_diff:.8f}")
                else:
                    print(f"   âœ… æ ‡ç­¾æ ‡å‡†åŒ–å™¨å‚æ•°åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­ä¿æŒä¸€è‡´")
        if hasattr(self, 'train_descriptor_stats') and self.train_descriptor_stats is not None:
            current_mean = self.descriptor_scaler.mean_ if hasattr(self.descriptor_scaler, 'mean_') else None
            current_scale = self.descriptor_scaler.scale_ if hasattr(self.descriptor_scaler, 'scale_') else None
            if current_mean is not None and current_scale is not None:
                mean_diff = np.mean(np.abs(current_mean - self.train_descriptor_stats['mean']))
                scale_diff = np.mean(np.abs(current_scale - self.train_descriptor_stats['scale']))
                if mean_diff > 1e-6 or scale_diff > 1e-6:
                    print(f"   âš ï¸  è­¦å‘Š: æè¿°ç¬¦æ ‡å‡†åŒ–å™¨å‚æ•°åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿå˜åŒ–")
                    print(f"     å¹³å‡å‡å€¼å·®å¼‚: {mean_diff:.8f}")
                    print(f"     å¹³å‡æ ‡å‡†å·®å·®å¼‚: {scale_diff:.8f}")
                else:
                    print(f"   âœ… æè¿°ç¬¦æ ‡å‡†åŒ–å™¨å‚æ•°åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­ä¿æŒä¸€è‡´")
        print("   ğŸ“Š é¢„æµ‹æ•°æ®ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
    def _verify_training_prediction_consistency(self, identifiers, normalized_predictions, lg_predictions):
        print("ğŸ” éªŒè¯è®­ç»ƒæ•°æ®é¢„æµ‹ä¸€è‡´æ€§...")
        train_labels_path = "labels_CH410.csv"
        if not os.path.exists(train_labels_path):
            print("   âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ ‡ç­¾æ–‡ä»¶ï¼Œæ— æ³•éªŒè¯è®­ç»ƒæ•°æ®é¢„æµ‹ä¸€è‡´æ€§")
            return
        try:
            train_labels_df = pd.read_csv(train_labels_path)
            train_sample_names = set(train_labels_df.iloc[:, 0].tolist())
            train_indices = []
            train_normalized_preds = []
            train_lg_preds = []
            for idx, identifier in enumerate(identifiers):
                if identifier in train_sample_names:
                    train_indices.append(idx)
                    train_normalized_preds.append(normalized_predictions[idx])
                    train_lg_preds.append(lg_predictions[idx])
            if train_indices:
                train_normalized_preds = np.array(train_normalized_preds)
                train_lg_preds = np.array(train_lg_preds)
                print(f"   ğŸ“Š è®­ç»ƒæ•°æ®é¢„æµ‹ç»Ÿè®¡:")
                print(f"     è®­ç»ƒæ ·æœ¬æ•°: {len(train_indices)}")
                print(f"     æ ‡å‡†åŒ–é¢„æµ‹å€¼èŒƒå›´: {train_normalized_preds.min():.4f} - {train_normalized_preds.max():.4f}")
                print(f"     æ ‡å‡†åŒ–é¢„æµ‹å€¼å‡å€¼: {np.mean(train_normalized_preds):.4f}")
                print(f"     æ ‡å‡†åŒ–é¢„æµ‹å€¼æ ‡å‡†å·®: {np.std(train_normalized_preds):.4f}")
                print(f"     lgé¢„æµ‹å€¼èŒƒå›´: {train_lg_preds.min():.4f} - {train_lg_preds.max():.4f}")
                print(f"     lgé¢„æµ‹å€¼å‡å€¼: {np.mean(train_lg_preds):.4f}")
                if hasattr(self, 'train_label_stats'):
                    expected_range = self.train_label_stats['original_range']
                    print(f"     è®­ç»ƒæ—¶æ ‡ç­¾èŒƒå›´: {expected_range[0]:.4f} - {expected_range[1]:.4f}")
                    if train_lg_preds.min() < expected_range[0] * 0.5 or train_lg_preds.max() > expected_range[1] * 2.0:
                        print(f"     âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®é¢„æµ‹å€¼è¶…å‡ºé¢„æœŸèŒƒå›´")
                    else:
                        print(f"     âœ… è®­ç»ƒæ•°æ®é¢„æµ‹å€¼åœ¨åˆç†èŒƒå›´å†…")
                print("   âœ… è®­ç»ƒæ•°æ®é¢„æµ‹ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
            else:
                print("   âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ ·æœ¬")
        except Exception as e:
            print(f"   âš ï¸  éªŒè¯è®­ç»ƒæ•°æ®é¢„æµ‹ä¸€è‡´æ€§æ—¶å‡ºé”™: {e}")
    def _check_batch_data_quality(self, batch, stats):
        if 'vae_data' in batch and isinstance(batch['vae_data'], torch.Tensor):
            if batch['vae_data'].abs().sum() == 0:
                stats['vae_data_zeros'] += 1
        if 'graph_cc' in batch:
            import dgl
            if isinstance(batch['graph_cc'], dgl.DGLHeteroGraph):
                if batch['graph_cc'].num_nodes('l') == 0 and batch['graph_cc'].num_nodes('n') == 0:
                    stats['gcn_data_empty'] += 1
        if 'lzhnn_data' in batch and isinstance(batch['lzhnn_data'], dict):
            lzhnn_data = batch['lzhnn_data']
            topo_sum = lzhnn_data.get('topo', torch.tensor(0))
            struct_sum = lzhnn_data.get('struct', torch.tensor(0))
            if isinstance(topo_sum, torch.Tensor):
                topo_sum = topo_sum.abs().sum()
            if isinstance(struct_sum, torch.Tensor):
                struct_sum = struct_sum.abs().sum()
            if topo_sum == 0 and struct_sum == 0:
                stats['lzhnn_data_zeros'] += 1
        if 'descriptors' in batch and isinstance(batch['descriptors'], torch.Tensor):
            if batch['descriptors'].abs().sum() == 0:
                stats['descriptors_zeros'] += 1
    def _prepare_batch_inputs(self, batch):
        inputs = {}
        if 'vae_data' in batch:
            inputs['vae_data'] = batch['vae_data'].to(self.device)
        if 'descriptors' in batch:
            inputs['descriptors'] = batch['descriptors'].to(self.device)
        if 'graph_cc' in batch:
            inputs['graph_cc'] = batch['graph_cc'].to(self.device)
        if 'graph_noncc' in batch:
            inputs['graph_noncc'] = batch['graph_noncc'].to(self.device)
        if 'lzhnn_data' in batch:
            lzhnn_data = batch['lzhnn_data']
            inputs['lzhnn_data'] = {
                'topo': lzhnn_data['topo'].to(self.device),
                'struct': lzhnn_data['struct'].to(self.device)
            }
        if 'cc_mask' in batch:
            inputs['cc_mask'] = batch['cc_mask'].to(self.device)
        return inputs
    def _init_label_scaler(self):
        try:
            from sklearn.preprocessing import StandardScaler
            self.label_scaler = StandardScaler()
            labels_path = "labels_CH410.csv"
            if os.path.exists(labels_path):
                labels_df = pd.read_csv(labels_path)
                print(f"ğŸ“Š è¯»å–è®­ç»ƒæ ‡ç­¾æ–‡ä»¶: {labels_path}")
                print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(labels_path)} bytes")
                print(f"   åˆ—å: {list(labels_df.columns)}")
                print(f"   æ ·æœ¬æ•°é‡: {len(labels_df)}")
                if 'target_property' in labels_df.columns:
                    original_labels = labels_df['target_property'].values
                    print(f"   æ ‡ç­¾ç»Ÿè®¡:")
                    print(f"     èŒƒå›´: {original_labels.min():.6f} - {original_labels.max():.6f}")
                    print(f"     å‡å€¼: {original_labels.mean():.6f}")
                    print(f"     æ ‡å‡†å·®: {original_labels.std():.6f}")
                    print(f"     NaNæ•°é‡: {np.isnan(original_labels).sum()}")
                    print(f"     æ— ç©·å¤§æ•°é‡: {np.isinf(original_labels).sum()}")
                    valid_mask = ~(np.isnan(original_labels) | np.isinf(original_labels))
                    if not valid_mask.all():
                        print(f"   âš ï¸  å‘ç° {np.sum(~valid_mask)} ä¸ªæ— æ•ˆæ ‡ç­¾å€¼ï¼Œå°†è¢«è¿‡æ»¤")
                        original_labels = original_labels[valid_mask]
                    if len(original_labels) > 0:
                        self.label_scaler.fit(original_labels.reshape(-1, 1))
                        print("âœ… æ ‡ç­¾æ ‡å‡†åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
                        print(f"   åŸºäºçœŸå®è®­ç»ƒæ•°æ®:")
                        print(f"   åŸå§‹æ ‡ç­¾èŒƒå›´: {original_labels.min():.6f} - {original_labels.max():.6f}")
                        print(f"   åŸå§‹æ ‡ç­¾å‡å€¼: {original_labels.mean():.6f}")
                        print(f"   åŸå§‹æ ‡ç­¾æ ‡å‡†å·®: {original_labels.std():.6f}")
                        standardized = self.label_scaler.transform(original_labels.reshape(-1, 1))
                        print(f"   æ ‡å‡†åŒ–åèŒƒå›´: {standardized.min():.6f} - {standardized.max():.6f}")
                        print(f"   æ ‡å‡†åŒ–åå‡å€¼: {standardized.mean():.6f}")
                        print(f"   æ ‡å‡†åŒ–åæ ‡å‡†å·®: {standardized.std():.6f}")
                        reconstructed = self.label_scaler.inverse_transform(standardized)
                        reconstruction_error = np.mean(np.abs(original_labels - reconstructed.flatten()))
                        print(f"   åæ ‡å‡†åŒ–è¯¯å·®: {reconstruction_error:.8f}")
                        if reconstruction_error > 1e-6:
                            print("   âš ï¸  åæ ‡å‡†åŒ–è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨æ•°å€¼ç²¾åº¦é—®é¢˜")
                        self.train_label_stats = {
                            'mean': self.label_scaler.mean_.copy(),
                            'scale': self.label_scaler.scale_.copy(),
                            'sample_count': len(original_labels),
                            'original_range': (original_labels.min(), original_labels.max()),
                            'original_mean': original_labels.mean(),
                            'original_std': original_labels.std()
                        }
                        print(f"   ğŸ“Š ä¿å­˜è®­ç»ƒæ—¶æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯:")
                        print(f"     åŸå§‹æ ‡ç­¾èŒƒå›´: {self.train_label_stats['original_range'][0]:.6f} - {self.train_label_stats['original_range'][1]:.6f}")
                        print(f"     åŸå§‹æ ‡ç­¾å‡å€¼: {self.train_label_stats['original_mean']:.6f}")
                        print(f"     åŸå§‹æ ‡ç­¾æ ‡å‡†å·®: {self.train_label_stats['original_std']:.6f}")
                    else:
                        raise ValueError("è¿‡æ»¤æ— æ•ˆå€¼åæ²¡æœ‰å‰©ä½™æ ‡ç­¾")
                else:
                    possible_cols = [col for col in labels_df.columns if 'target' in col.lower() or 'property' in col.lower() or 'label' in col.lower()]
                    if possible_cols:
                        print(f"   âš ï¸  æœªæ‰¾åˆ°target_propertyåˆ—ï¼Œå°è¯•ä½¿ç”¨: {possible_cols}")
                        col_name = possible_cols[0]
                        original_labels = labels_df[col_name].values
                        self.label_scaler.fit(original_labels.reshape(-1, 1))
                        print(f"   âœ… ä½¿ç”¨åˆ— {col_name} åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨")
                    else:
                        raise ValueError(f"labels.csvä¸­æ²¡æœ‰æ‰¾åˆ°target_propertyåˆ—æˆ–å…¶ä»–ç›¸å…³åˆ—")
            else:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°labels.csvæ–‡ä»¶: {labels_path}")
        except Exception as e:
            print(f"âš ï¸  æ ‡ç­¾æ ‡å‡†åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"   ä½¿ç”¨é»˜è®¤è®¾ç½®...")
            self.label_scaler = StandardScaler()
            self.label_scaler.mean_ = np.array([1.85])
            self.label_scaler.scale_ = np.array([0.75])
            print(f"   é»˜è®¤è®¾ç½®: å‡å€¼=1.85, æ ‡å‡†å·®=0.75")
            print(f"   âš ï¸  è­¦å‘Š: ä½¿ç”¨é»˜è®¤è®¾ç½®å¯èƒ½å¯¼è‡´é¢„æµ‹ç²¾åº¦ä¸‹é™")
    def _init_descriptor_scaler(self):
        try:
            descriptor_path = "VAE/descriptor.csv"
            if os.path.exists(descriptor_path):
                descriptor_df = pd.read_csv(descriptor_path)
                print(f"ğŸ“Š è¯»å–è®­ç»ƒæè¿°ç¬¦æ–‡ä»¶: {descriptor_path}")
                print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(descriptor_path)} bytes")
                print(f"   åˆ—å: {list(descriptor_df.columns)}")
                print(f"   æ ·æœ¬æ•°é‡: {len(descriptor_df)}")
                descriptor_cols = [col for col in descriptor_df.columns if col != 'name']
                descriptor_values = descriptor_df[descriptor_cols].values
                print(f"   æè¿°ç¬¦ç»Ÿè®¡:")
                print(f"     ç‰¹å¾æ•°é‡: {len(descriptor_cols)}")
                print(f"     æ•°æ®å½¢çŠ¶: {descriptor_values.shape}")
                print(f"     NaNæ•°é‡: {np.isnan(descriptor_values).sum()}")
                print(f"     æ— ç©·å¤§æ•°é‡: {np.isinf(descriptor_values).sum()}")
                valid_mask = ~(np.isnan(descriptor_values).any(axis=1) | np.isinf(descriptor_values).any(axis=1))
                if not valid_mask.all():
                    print(f"   âš ï¸  å‘ç° {np.sum(~valid_mask)} ä¸ªæ— æ•ˆæ ·æœ¬ï¼Œå°†è¢«è¿‡æ»¤")
                    descriptor_values = descriptor_values[valid_mask]
                if len(descriptor_values) > 0:
                    from sklearn.preprocessing import StandardScaler
                    self.descriptor_scaler = StandardScaler()
                    self.descriptor_scaler.fit(descriptor_values)
                    self.descriptor_cols = descriptor_cols
                    print("âœ… æè¿°ç¬¦æ ‡å‡†åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
                    print(f"   æè¿°ç¬¦ç‰¹å¾æ•°é‡: {len(self.descriptor_cols)}")
                    print(f"   è®­ç»ƒæ—¶åˆ—å: {self.descriptor_cols}")
                    standardized = self.descriptor_scaler.transform(descriptor_values)
                    print(f"   æ ‡å‡†åŒ–åç»Ÿè®¡:")
                    print(f"     å‡å€¼èŒƒå›´: {standardized.mean(axis=0).min():.6f} - {standardized.mean(axis=0).max():.6f}")
                    print(f"     æ ‡å‡†å·®èŒƒå›´: {standardized.std(axis=0).min():.6f} - {standardized.std(axis=0).max():.6f}")
                    reconstructed = self.descriptor_scaler.inverse_transform(standardized)
                    reconstruction_error = np.mean(np.abs(descriptor_values - reconstructed))
                    print(f"     åæ ‡å‡†åŒ–è¯¯å·®: {reconstruction_error:.8f}")
                    if reconstruction_error > 1e-6:
                        print("   âš ï¸  åæ ‡å‡†åŒ–è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨æ•°å€¼ç²¾åº¦é—®é¢˜")
                    self.train_descriptor_stats = {
                        'mean': self.descriptor_scaler.mean_.copy(),
                        'scale': self.descriptor_scaler.scale_.copy(),
                        'sample_count': len(descriptor_values)
                    }
                    print(f"   ğŸ“Š ä¿å­˜è®­ç»ƒæ—¶æè¿°ç¬¦ç»Ÿè®¡ä¿¡æ¯:")
                    print(f"     å‡å€¼èŒƒå›´: {self.train_descriptor_stats['mean'].min():.6f} - {self.train_descriptor_stats['mean'].max():.6f}")
                    print(f"     æ ‡å‡†å·®èŒƒå›´: {self.train_descriptor_stats['scale'].min():.6f} - {self.train_descriptor_stats['scale'].max():.6f}")
                else:
                    raise ValueError("è¿‡æ»¤æ— æ•ˆå€¼åæ²¡æœ‰å‰©ä½™æ ·æœ¬")
            else:
                print("âš ï¸  è®­ç»ƒæè¿°ç¬¦æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡æè¿°ç¬¦æ ‡å‡†åŒ–")
                self.descriptor_scaler = None
                self.descriptor_cols = None
        except Exception as e:
            print(f"âš ï¸  æè¿°ç¬¦æ ‡å‡†åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.descriptor_scaler = None
            self.descriptor_cols = None
    def _denormalize_predictions(self, predictions):
        if self.label_scaler is not None:
            try:
                predictions_2d = predictions.reshape(-1, 1)
                lg_values = self.label_scaler.inverse_transform(predictions_2d)
                print(f"    ğŸ“Š åæ ‡å‡†åŒ–å®Œæˆï¼ˆä¿ç•™lgå€¼ï¼‰:")
                print(f"       æ ‡å‡†åŒ–å€¼èŒƒå›´: {predictions.min():.4f} - {predictions.max():.4f}")
                print(f"       lgå€¼èŒƒå›´: {lg_values.min():.4f} - {lg_values.max():.4f}")
                print(f"       æ³¨æ„ï¼šè¿”å›çš„æ˜¯lgå€¼ï¼Œä¸æ˜¯åŸå§‹å€¼")
                return lg_values.flatten()
            except Exception as e:
                print(f"âš ï¸  åæ ‡å‡†åŒ–å¤±è´¥: {e}")
                return predictions
        else:
            print("âš ï¸  æ ‡ç­¾æ ‡å‡†åŒ–å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡åæ ‡å‡†åŒ–")
            return predictions
class CustomDataset(Dataset):
    def __init__(self, data_config, linkers_csv, descriptor_scaler, descriptor_cols, use_parallel=True, num_workers=None):
        self.data_config = data_config
        self.linkers_csv = linkers_csv
        self.descriptor_scaler = descriptor_scaler
        self.descriptor_cols = descriptor_cols
        self.use_parallel = use_parallel
        self.num_workers = num_workers
        self.descriptor_df = pd.read_csv(data_config['descriptor_csv'])
        self.struct_features_df = pd.read_csv(data_config['struct_features_csv'])
        self.sample_identifiers = self._get_sample_identifiers()
        print("ğŸ” ç»Ÿä¸€æå–å’Œç¼“å­˜æ‹“æ‰‘ç‰¹å¾...")
        self.topo_features_cache = {}
        self._extract_and_cache_all_features()
        print(f"æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.sample_identifiers)} ä¸ªæ ·æœ¬")
        self._verify_dataset_consistency()
    def _verify_dataset_consistency(self):
        print("ğŸ” éªŒè¯æ•°æ®é›†ç‰¹å¾æå–ä¸€è‡´æ€§...")
        train_labels_path = "labels_CH410.csv"
        if os.path.exists(train_labels_path):
            try:
                train_labels_df = pd.read_csv(train_labels_path)
                train_sample_names = set(train_labels_df.iloc[:, 0].tolist())
                train_samples = set(self.sample_identifiers).intersection(train_sample_names)
                if train_samples:
                    print(f"   ğŸ“Š è®­ç»ƒæ•°æ®ç‰¹å¾è´¨é‡æ£€æŸ¥:")
                    print(f"     è®­ç»ƒæ ·æœ¬æ•°: {len(train_samples)}")
                    train_topo_nonzero = sum(1 for name in train_samples if self._get_topo_sum(name) > 0)
                    print(f"     æ‹“æ‰‘ç‰¹å¾éé›¶æ ·æœ¬: {train_topo_nonzero}/{len(train_samples)} ({train_topo_nonzero/len(train_samples)*100:.1f}%)")
                    if hasattr(self, 'struct_features_df'):
                        struct_cols = [col for col in self.struct_features_df.columns if col != 'name']
                        if struct_cols:
                            struct_values = self.struct_features_df[self.struct_features_df.iloc[:, 0].isin(train_samples)][struct_cols].values
                            struct_nonzero = np.sum(struct_values != 0)
                            struct_total = struct_values.size
                            print(f"     ç»“æ„ç‰¹å¾éé›¶å€¼: {struct_nonzero}/{struct_total} ({struct_nonzero/struct_total*100:.1f}%)")
                    if hasattr(self, 'descriptor_df'):
                        desc_cols = [col for col in self.descriptor_df.columns if col != 'name']
                        if desc_cols:
                            desc_values = self.descriptor_df[self.descriptor_df['name'].isin(train_samples)][desc_cols].values
                            desc_nonzero = np.sum(desc_values != 0)
                            desc_total = desc_values.size
                            print(f"     æè¿°ç¬¦éé›¶å€¼: {desc_nonzero}/{desc_total} ({desc_nonzero/desc_total*100:.1f}%)")
                    print("   âœ… è®­ç»ƒæ•°æ®ç‰¹å¾è´¨é‡æ£€æŸ¥å®Œæˆ")
                else:
                    print("   âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ ·æœ¬")
            except Exception as e:
                print(f"   âš ï¸  éªŒè¯æ•°æ®é›†ä¸€è‡´æ€§æ—¶å‡ºé”™: {e}")
        print("   ğŸ“Š æ•°æ®é›†ç‰¹å¾æå–ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
    def _extract_and_cache_all_features(self):
        try:
            cache_dir = "feature_cache"
            topo_cache_file = os.path.join(cache_dir, "topo_features_cache.pkl")
            print(f"ğŸ” æ£€æŸ¥æ‹“æ‰‘ç‰¹å¾ç¼“å­˜...")
            print(f"   ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"   ç¼“å­˜æ–‡ä»¶: {topo_cache_file}")
            if os.path.exists(topo_cache_file):
                try:
                    print("   ğŸ“¥ å‘ç°ç°æœ‰ç¼“å­˜ï¼Œå°è¯•åŠ è½½...")
                    with open(topo_cache_file, 'rb') as f:
                        cached_data = pickle.load(f)
                    cache_version = cached_data.get('version', '1.0')
                    cache_timestamp = cached_data.get('timestamp', 0)
                    cached_features = cached_data.get('features', {})
                    print(f"   ğŸ“Š ç¼“å­˜ä¿¡æ¯:")
                    print(f"     ç‰ˆæœ¬: {cache_version}")
                    print(f"     åˆ›å»ºæ—¶é—´: {pd.Timestamp.fromtimestamp(cache_timestamp)}")
                    print(f"     ç¼“å­˜æ ·æœ¬æ•°: {len(cached_features)}")
                    current_time = time.time()
                    cache_age_days = (current_time - cache_timestamp) / (24 * 3600)
                    if cache_age_days > 7:
                        print(f"   âš ï¸  ç¼“å­˜å·²è¿‡æœŸ ({cache_age_days:.1f} å¤©)ï¼Œå°†é‡æ–°ç”Ÿæˆ")
                        use_cache = False
                    else:
                        print(f"   âœ… ç¼“å­˜æœ‰æ•ˆ ({cache_age_days:.1f} å¤©)")
                        use_cache = True
                    missing_samples = set(self.sample_identifiers) - set(cached_features.keys())
                    if missing_samples:
                        print(f"   âš ï¸  ç¼“å­˜ä¸­ç¼ºå°‘ {len(missing_samples)} ä¸ªæ ·æœ¬ï¼Œå°†è¡¥å……æå–")
                        use_cache = True  
                    else:
                        print(f"   âœ… ç¼“å­˜åŒ…å«æ‰€æœ‰æ ·æœ¬")
                    if use_cache:
                        for sample_name in self.sample_identifiers:
                            if sample_name in cached_features:
                                feat = cached_features[sample_name]
                                if isinstance(feat, np.ndarray):
                                    self.topo_features_cache[sample_name] = torch.from_numpy(feat).float()
                                else:
                                    self.topo_features_cache[sample_name] = feat
                            else:
                                print(f"      ğŸ”„ é‡æ–°æå–ç¼ºå¤±æ ·æœ¬: {sample_name}")
                                topo_features = self._extract_real_topo_features(sample_name)
                                self.topo_features_cache[sample_name] = topo_features
                        print(f"   âœ… ä»ç¼“å­˜åŠ è½½å®Œæˆï¼Œå…± {len(self.topo_features_cache)} ä¸ªæ ·æœ¬")
                        cached_count = sum(1 for name in self.sample_identifiers if name in cached_features)
                        cache_hit_rate = cached_count / len(self.sample_identifiers) * 100
                        print(f"   ğŸ“Š ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1f}% ({cached_count}/{len(self.sample_identifiers)})")
                        return  
                except Exception as e:
                    print(f"   âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
                    print("   ğŸ”„ å°†é‡æ–°æå–æ‰€æœ‰ç‰¹å¾")
            print("   ğŸ”„ å¼€å§‹ç‰¹å¾æå–...")
            mol2vec_path = "PP-NN/models/mol2vec_300dim.pkl"
            if not os.path.exists(mol2vec_path):
                print(f"âš ï¸  mol2vecæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {mol2vec_path}")
                alternative_paths = [
                    "PP-NN/models/mol2vec_300dim.kv",
                    "PP-NN/models/mol2vec_300dim.pkl",
                    "./PP-NN/models/mol2vec_300dim.pkl",
                    "../PP-NN/models/mol2vec_300dim.pkl"
                ]
                mol2vec_found = False
                for alt_path in alternative_paths:
                    if os.path.exists(alt_path):
                        print(f"âœ… æ‰¾åˆ°mol2vecæ¨¡å‹: {alt_path}")
                        mol2vec_path = alt_path
                        mol2vec_found = True
                        break
                if not mol2vec_found:
                    print(f"âŒ æœªæ‰¾åˆ°mol2vecæ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é›¶ç‰¹å¾")
                    for sample_name in self.sample_identifiers:
                        self.topo_features_cache[sample_name] = torch.zeros(18, dtype=torch.float32)
                    return
            os.environ['MOL2VEC_MODEL_PATH'] = mol2vec_path
            print(f"   ğŸ”§ è®¾ç½®mol2vecæ¨¡å‹è·¯å¾„: {mol2vec_path}")
            print("   ğŸ”§ ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„LZHNNç‰¹å¾æå–æ–¹æ³•")
            print("   ğŸ“ æ³¨æ„ï¼šèåˆæ¨¡å‹ä¸­çš„LZHNNä½¿ç”¨çœŸå®çš„æŒä¹…åŒè°ƒç‰¹å¾(18ç»´)å’Œç»“æ„ç‰¹å¾(5ç»´)")
            struct_features_csv = self.data_config['struct_features_csv']
            cif_dir = self.data_config['cif_dir']
            if not os.path.exists(struct_features_csv):
                print(f"   âŒ ç»“æ„ç‰¹å¾CSVæ–‡ä»¶ä¸å­˜åœ¨: {struct_features_csv}")
                for sample_name in self.sample_identifiers:
                    self.topo_features_cache[sample_name] = torch.zeros(18, dtype=torch.float32)
                return
            if not os.path.exists(cif_dir):
                print(f"   âŒ CIFç›®å½•ä¸å­˜åœ¨: {cif_dir}")
                for sample_name in self.sample_identifiers:
                    self.topo_features_cache[sample_name] = torch.zeros(18, dtype=torch.float32)
                return
            print(f"   âœ… ç»“æ„ç‰¹å¾CSVæ–‡ä»¶å­˜åœ¨: {struct_features_csv}")
            print(f"   âœ… CIFç›®å½•å­˜åœ¨: {cif_dir}")
            print("   ğŸ”§ å°†ä½¿ç”¨çœŸå®çš„æŒä¹…åŒè°ƒç‰¹å¾æå–æ–¹æ³•")
            print("ğŸ” åˆ†ç¦»è®­ç»ƒæ•°æ®å’Œæ–°æ•°æ®...")
            train_labels_path = "labels_CH410.csv"
            if os.path.exists(train_labels_path):
                train_labels_df = pd.read_csv(train_labels_path)
                train_sample_names = set(train_labels_df.iloc[:, 0].tolist())  
                print(f"   è®­ç»ƒæ—¶æ ·æœ¬æ•°: {len(train_sample_names)}")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒæ ‡ç­¾æ–‡ä»¶ï¼Œæ— æ³•åŒºåˆ†è®­ç»ƒæ•°æ®å’Œæ–°æ•°æ®")
                train_sample_names = set()
            current_samples = set(self.sample_identifiers)
            train_samples = current_samples.intersection(train_sample_names)
            new_samples = current_samples - train_sample_names
            print(f"   å½“å‰æ ·æœ¬æ€»æ•°: {len(current_samples)}")
            print(f"   è®­ç»ƒæ•°æ®æ ·æœ¬æ•°: {len(train_samples)}")
            print(f"   æ–°æ•°æ®æ ·æœ¬æ•°: {len(new_samples)}")
            print("ğŸ“Š å¼€å§‹ç‰¹å¾æå–...")
            if train_samples:
                print(f"   1ï¸âƒ£ å¤„ç†è®­ç»ƒæ•°æ® ({len(train_samples)} ä¸ªæ ·æœ¬)...")
                if self.use_parallel:
                    print("   ğŸš€ å¯åŠ¨å¹¶è¡Œç‰¹å¾æå–...")
                    train_topo_features = self._extract_topo_features_parallel(sorted(train_samples), self.num_workers)
                else:
                    print("   ğŸ”„ å¯åŠ¨ä¸²è¡Œç‰¹å¾æå–...")
                    train_topo_features = self._extract_topo_features_serial(sorted(train_samples))
                for sample_name, features in train_topo_features.items():
                    self.topo_features_cache[sample_name] = features
                print("   âœ… è®­ç»ƒæ•°æ®ç‰¹å¾æå–å®Œæˆ")
            if new_samples:
                print(f"   2ï¸âƒ£ å¤„ç†æ–°æ•°æ® ({len(new_samples)} ä¸ªæ ·æœ¬)...")
                print("   ğŸ“ æ³¨æ„ï¼šæ–°æ•°æ®ä½¿ç”¨ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒçš„ç‰¹å¾æå–æ–¹æ³•")
                if self.use_parallel:
                    print("   ğŸš€ å¯åŠ¨å¹¶è¡Œç‰¹å¾æå–...")
                    new_topo_features = self._extract_topo_features_parallel(sorted(new_samples), self.num_workers)
                else:
                    print("   ğŸ”„ å¯åŠ¨ä¸²è¡Œç‰¹å¾æå–...")
                    new_topo_features = self._extract_topo_features_serial(sorted(new_samples))
                for sample_name, features in new_topo_features.items():
                    self.topo_features_cache[sample_name] = features
                print("   âœ… æ–°æ•°æ®ç‰¹å¾æå–å®Œæˆ")
            print("âœ… æ‰€æœ‰ç‰¹å¾æå–å’Œç¼“å­˜å®Œæˆ")
            self._save_features_to_cache()
            total_samples = len(self.sample_identifiers)
            topo_nonzero_count = sum(1 for feat in self.topo_features_cache.values() if self._get_feat_sum(feat) > 0)
            print(f"ğŸ“Š ç‰¹å¾è´¨é‡ç»Ÿè®¡:")
            print(f"   æ‹“æ‰‘ç‰¹å¾éé›¶æ ·æœ¬: {topo_nonzero_count}/{total_samples} ({topo_nonzero_count/total_samples*100:.1f}%)")
            if train_samples:
                print("ğŸ” éªŒè¯è®­ç»ƒæ•°æ®ç‰¹å¾ä¸€è‡´æ€§...")
                train_topo_nonzero = sum(1 for name in train_samples if self._get_topo_sum(name) > 0)
                print(f"   è®­ç»ƒæ•°æ®æ‹“æ‰‘ç‰¹å¾éé›¶: {train_topo_nonzero}/{len(train_samples)} ({train_topo_nonzero/len(train_samples)*100:.1f}%)")
                if train_topo_nonzero == 0:
                    print("   âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®ç‰¹å¾å¯èƒ½å­˜åœ¨é—®é¢˜")
                else:
                    print("   âœ… è®­ç»ƒæ•°æ®ç‰¹å¾ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        except Exception as e:
            print(f"âš ï¸  ç‰¹å¾æå–å’Œç¼“å­˜è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            for sample_name in self.sample_identifiers:
                self.topo_features_cache[sample_name] = torch.zeros(18, dtype=torch.float32)
    def _save_features_to_cache(self):
        try:
            cache_dir = "feature_cache"
            os.makedirs(cache_dir, exist_ok=True)
            cache_data = {
                'version': '1.0',
                'timestamp': time.time(),
                'features': {},
                'metadata': {
                    'total_samples': len(self.topo_features_cache),
                    'feature_dim': 18,
                    'data_type': 'topological_features',
                    'extraction_method': 'persistent_homology'
                }
            }
            for sample_name, features in self.topo_features_cache.items():
                if isinstance(features, torch.Tensor):
                    cache_data['features'][sample_name] = features.cpu().numpy()
                else:
                    cache_data['features'][sample_name] = features
            cache_file = os.path.join(cache_dir, "topo_features_cache.pkl")
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            cache_size = os.path.getsize(cache_file) / (1024 * 1024)  
            print(f"ğŸ’¾ ç‰¹å¾ç¼“å­˜å·²ä¿å­˜:")
            print(f"   ç¼“å­˜æ–‡ä»¶: {cache_file}")
            print(f"   ç¼“å­˜å¤§å°: {cache_size:.2f} MB")
            print(f"   ç¼“å­˜æ ·æœ¬æ•°: {len(self.topo_features_cache)}")
            print(f"   ä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨åŠ è½½æ­¤ç¼“å­˜")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç‰¹å¾ç¼“å­˜å¤±è´¥: {e}")
            print("   ç‰¹å¾ä»ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œä½†ä¸ä¼šæŒä¹…åŒ–")
    def _load_features_from_cache(self, cache_file):
        try:
            with open(cache_file, 'rb') as f:
                cached_data = pickle.load(f)
            if not isinstance(cached_data, dict):
                raise ValueError("ç¼“å­˜æ–‡ä»¶æ ¼å¼é”™è¯¯")
            features = cached_data.get('features', {})
            if not features:
                raise ValueError("ç¼“å­˜æ–‡ä»¶ä¸­æ²¡æœ‰ç‰¹å¾æ•°æ®")
            loaded_features = {}
            for sample_name, feature_array in features.items():
                if isinstance(feature_array, np.ndarray):
                    loaded_features[sample_name] = torch.from_numpy(feature_array).float()
                else:
                    loaded_features[sample_name] = feature_array
            return loaded_features
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç‰¹å¾ç¼“å­˜å¤±è´¥: {e}")
            return {}
    def _extract_real_topo_features(self, sample_name):
        try:
            parts = sample_name.split('_')
            if len(parts) >= 4:
                topo_type = parts[-4]  
            else:
                topo_type = "default"
            cif_dir = self.data_config['cif_dir']
            cif_files = list(Path(cif_dir).glob(f"{sample_name}*.cif"))
            if not cif_files:
                return torch.zeros(18, dtype=torch.float32)
            cif_file = cif_files[0]
            try:
                from pymatgen.core import Structure
                import gudhi as gd
                struct = Structure.from_file(str(cif_file))
                coords = struct.cart_coords
                rips = gd.RipsComplex(points=coords, max_edge_length=10.0)
                simplex_tree = rips.create_simplex_tree(max_dimension=2)
                diag = simplex_tree.persistence(homology_coeff_field=2, min_persistence=0.1)
                pers_0 = [p[1][1] - p[1][0] for p in diag if p[0] == 0 and p[1][1] != float('inf')]
                pers_1 = [p[1][1] - p[1][1] for p in diag if p[0] == 1 and p[1][1] != float('inf')]
                bins = np.linspace(0, 5, 10)  
                hist_0 = np.histogram(pers_0, bins=bins)[0] if pers_0 else np.zeros(9)
                hist_1 = np.histogram(pers_1, bins=bins)[0] if pers_1 else np.zeros(9)
                pers = np.concatenate([hist_0, hist_1]).astype(np.float32)
                return torch.tensor(pers, dtype=torch.float32)
            except ImportError as e:
                return torch.zeros(18, dtype=torch.float32)
        except Exception as e:
            return torch.zeros(18, dtype=torch.float32)
    def _extract_topo_features_parallel(self, sample_names, num_workers=None):
        if num_workers is None:
            num_workers = max(1, multiprocessing.cpu_count() - 1)
        print(f"ğŸš€ å¯åŠ¨å¤šæ ¸å¹¶è¡Œç‰¹å¾æå–...")
        print(f"   ğŸ“Š å¹¶è¡Œè¿›ç¨‹æ•°: {num_workers}")
        print(f"   ğŸ“Š æ€»æ ·æœ¬æ•°: {len(sample_names)}")
        cif_dir = self.data_config['cif_dir']
        pbar = tqdm(total=len(sample_names), desc="æå–æ‹“æ‰‘ç‰¹å¾", unit="æ ·æœ¬")
        start_time = time.time()
        completed_count = 0
        failed_count = 0
        results = {}
        batch_size = min(100, max(10, len(sample_names) // num_workers))
        print(f"   ğŸ“Š æ‰¹å¤„ç†å¤§å°: {batch_size}")
        try:
            for batch_start in range(0, len(sample_names), batch_size):
                batch_end = min(batch_start + batch_size, len(sample_names))
                batch_samples = sample_names[batch_start:batch_end]
                print(f"   ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(sample_names)-1)//batch_size + 1}: {len(batch_samples)} ä¸ªæ ·æœ¬")
                with ProcessPoolExecutor(max_workers=num_workers) as executor:
                    future_to_sample = {}
                    for sample_name in batch_samples:
                        future = executor.submit(self._extract_single_topo_worker, sample_name, cif_dir)
                        future_to_sample[future] = sample_name
                    for future in as_completed(future_to_sample):
                        sample_name = future_to_sample[future]
                        try:
                            result = future.result(timeout=300)  
                            results[sample_name] = result
                            completed_count += 1
                            pbar.update(1)
                            if completed_count > 0:
                                elapsed_time = time.time() - start_time
                                avg_time_per_sample = elapsed_time / completed_count
                                remaining_samples = len(sample_names) - completed_count
                                estimated_remaining_time = avg_time_per_sample * remaining_samples
                                pbar.set_description(
                                    f"æå–æ‹“æ‰‘ç‰¹å¾ ({completed_count}/{len(sample_names)}) "
                                    f"é¢„è®¡å‰©ä½™: {estimated_remaining_time/60:.1f}åˆ†é’Ÿ"
                                )
                        except Exception as e:
                            failed_count += 1
                            print(f"âš ï¸  æ ·æœ¬ {sample_name} ç‰¹å¾æå–å¤±è´¥: {e}")
                            results[sample_name] = torch.zeros(18, dtype=torch.float32)
                            completed_count += 1
                            pbar.update(1)
                            if failed_count > len(sample_names) * 0.1:  
                                print(f"âš ï¸  å¤±è´¥ç‡è¿‡é«˜ ({failed_count}/{completed_count})ï¼Œè€ƒè™‘å›é€€åˆ°ä¸²è¡Œå¤„ç†")
                import gc
                gc.collect()
                time.sleep(1)
            pbar.close()
            total_time = time.time() - start_time
            avg_time_per_sample = total_time / len(sample_names)
            print(f"âœ… å¹¶è¡Œç‰¹å¾æå–å®Œæˆ!")
            print(f"   ğŸ“Š æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
            print(f"   ğŸ“Š å¹³å‡æ¯æ ·æœ¬: {avg_time_per_sample:.2f} ç§’")
            print(f"   ğŸ“Š å¹¶è¡ŒåŠ é€Ÿæ¯”: {num_workers:.1f}x")
            print(f"   ğŸ“Š æˆåŠŸæ ·æœ¬: {completed_count - failed_count}")
            print(f"   ğŸ“Š å¤±è´¥æ ·æœ¬: {failed_count}")
            print(f"   ğŸ“Š æˆåŠŸç‡: {(completed_count - failed_count)/len(sample_names)*100:.1f}%")
            return results
        except Exception as e:
            print(f"âŒ å¹¶è¡Œç‰¹å¾æå–å¤±è´¥: {e}")
            pbar.close()
            print("ğŸ”„ å›é€€åˆ°ä¸²è¡Œå¤„ç†...")
            return self._extract_topo_features_serial(sample_names)
    def _extract_single_topo_worker(self, sample_name, cif_dir):
        try:
            import resource
            import signal
            try:
                resource.setrlimit(resource.RLIMIT_AS, (1024 * 1024 * 1024, -1))
            except:
                pass
            try:
                resource.setrlimit(resource.RLIMIT_CPU, (600, -1))
            except:
                pass
            parts = sample_name.split('_')
            if len(parts) >= 4:
                topo_type = parts[-4]
            else:
                topo_type = "default"
            cif_files = list(Path(cif_dir).glob(f"{sample_name}*.cif"))
            if not cif_files:
                return torch.zeros(18, dtype=torch.float32)
            cif_file = cif_files[0]
            from pymatgen.core import Structure
            import gudhi as gd
            file_size = cif_file.stat().st_size
            if file_size > 10 * 1024 * 1024:  
                print(f"      âš ï¸  CIFæ–‡ä»¶è¿‡å¤§: {file_size/(1024*1024):.1f}MBï¼Œè·³è¿‡")
                return torch.zeros(18, dtype=torch.float32)
            struct = Structure.from_file(str(cif_file))
            coords = struct.cart_coords
            if len(coords) > 10000:  
                print(f"      âš ï¸  ç»“æ„è¿‡äºå¤æ‚: {len(coords)} ä¸ªåŸå­ï¼Œè·³è¿‡")
                return torch.zeros(18, dtype=torch.float32)
            rips = gd.RipsComplex(points=coords, max_edge_length=10.0)
            simplex_tree = rips.create_simplex_tree(max_dimension=2)
            diag = simplex_tree.persistence(homology_coeff_field=2, min_persistence=0.1)
            pers_0 = [p[1][1] - p[1][0] for p in diag if p[0] == 0 and p[1][1] != float('inf')]
            pers_1 = [p[1][1] - p[1][0] for p in diag if p[0] == 1 and p[1][1] != float('inf')]
            bins = np.linspace(0, 5, 10)
            hist_0 = np.histogram(pers_0, bins=bins)[0] if pers_0 else np.zeros(9)
            hist_1 = np.histogram(pers_1, bins=bins)[0] if pers_1 else np.zeros(9)
            pers = np.concatenate([hist_0, hist_1]).astype(np.float32)
            del struct, coords, rips, simplex_tree, diag
            import gc
            gc.collect()
            return torch.tensor(pers, dtype=torch.float32)
        except Exception as e:
            error_msg = str(e)
            if "memory" in error_msg.lower() or "out of memory" in error_msg.lower():
                print(f"      âŒ å†…å­˜ä¸è¶³: {sample_name}")
            elif "timeout" in error_msg.lower():
                print(f"      â° è¶…æ—¶: {sample_name}")
            elif "process" in error_msg.lower():
                print(f"      ğŸ”„ è¿›ç¨‹å¼‚å¸¸: {sample_name}")
            else:
                print(f"      âŒ å…¶ä»–é”™è¯¯: {sample_name} - {error_msg}")
            return torch.zeros(18, dtype=torch.float32)
    def _extract_topo_features_serial(self, sample_names):
        print(f"ğŸ”„ ä¸²è¡Œç‰¹å¾æå–...")
        results = {}
        pbar = tqdm(total=len(sample_names), desc="ä¸²è¡Œæå–æ‹“æ‰‘ç‰¹å¾", unit="æ ·æœ¬")
        start_time = time.time()
        failed_count = 0
        for i, sample_name in enumerate(sample_names):
            try:
                result = self._extract_real_topo_features(sample_name)
                results[sample_name] = result
                pbar.update(1)
                if i > 0:
                    elapsed_time = time.time() - start_time
                    avg_time_per_sample = elapsed_time / (i + 1)
                    remaining_samples = len(sample_names) - (i + 1)
                    estimated_remaining_time = avg_time_per_sample * remaining_samples
                    pbar.set_description(
                        f"ä¸²è¡Œæå–æ‹“æ‰‘ç‰¹å¾ ({i+1}/{len(sample_names)}) "
                        f"é¢„è®¡å‰©ä½™: {estimated_remaining_time/60:.1f}åˆ†é’Ÿ"
                    )
                if i % 100 == 0:
                    import gc
                    gc.collect()
            except Exception as e:
                failed_count += 1
                print(f"âš ï¸  æ ·æœ¬ {sample_name} ç‰¹å¾æå–å¤±è´¥: {e}")
                results[sample_name] = torch.zeros(18, dtype=torch.float32)
                pbar.update(1)
                if failed_count > 50:
                    print(f"âš ï¸  è¿ç»­å¤±è´¥è¿‡å¤šï¼Œè€ƒè™‘è·³è¿‡å‰©ä½™æ ·æœ¬")
                    break
        pbar.close()
        total_time = time.time() - start_time
        print(f"âœ… ä¸²è¡Œç‰¹å¾æå–å®Œæˆ!")
        print(f"   ğŸ“Š æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"   ğŸ“Š æˆåŠŸæ ·æœ¬: {len(sample_names) - failed_count}")
        print(f"   ğŸ“Š å¤±è´¥æ ·æœ¬: {failed_count}")
        print(f"   ğŸ“Š æˆåŠŸç‡: {(len(sample_names) - failed_count)/len(sample_names)*100:.1f}%")
        return results
    def _get_sample_identifiers(self):
        descriptor_names = set(self.descriptor_df['name'].tolist())
        struct_names = set(self.struct_features_df.iloc[:, 0].tolist())
        common_names = descriptor_names.intersection(struct_names)
        print(f"æè¿°ç¬¦æ ·æœ¬æ•°: {len(descriptor_names)}")
        print(f"ç»“æ„ç‰¹å¾æ ·æœ¬æ•°: {len(struct_names)}")
        print(f"å…±åŒæ ·æœ¬æ•°: {len(common_names)}")
        if len(common_names) == 0:
            print("âŒ é”™è¯¯: æè¿°ç¬¦å’Œç»“æ„ç‰¹å¾CSVä¸­æ²¡æœ‰å…±åŒæ ·æœ¬")
            print("   è¯·æ£€æŸ¥ä¸¤ä¸ªCSVæ–‡ä»¶çš„æ ·æœ¬åç§°æ˜¯å¦ä¸€è‡´")
            return []
        gcn_cache_dir = self.data_config['gcn_cache_dir']
        samples_with_gcn = []
        samples_without_gcn = []
        print("ğŸ” æ£€æŸ¥GCNç¼“å­˜å¯ç”¨æ€§...")
        print(f"   ç¼“å­˜ç›®å½•: {gcn_cache_dir}")
        print(f"   ç¼“å­˜ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(gcn_cache_dir)}")
        if os.path.exists(gcn_cache_dir):
            cache_files = list(Path(gcn_cache_dir).glob('*.pkl'))
            print(f"   ç¼“å­˜ç›®å½•ä¸­çš„.pklæ–‡ä»¶æ•°é‡: {len(cache_files)}")
            if cache_files:
                print(f"   å‰5ä¸ªç¼“å­˜æ–‡ä»¶: {[f.name for f in cache_files[:5]]}")
        test_samples = list(common_names)[:5]
        print(f"   æµ‹è¯•å‰5ä¸ªæ ·æœ¬çš„ç¼“å­˜æƒ…å†µ:")
        for sample_name in test_samples:
            possible_cache_names = [
                f"{sample_name}.pkl",  
                f"{sample_name}.cif.pkl",  
                f"{sample_name}_relaxed.pkl",  
                f"{sample_name}_relaxed_interp_2.pkl",  
                f"{sample_name}_relaxed_interp_3.pkl"   
            ]
            cache_found = False
            actual_cache_name = None
            for cache_name in possible_cache_names:
                cache_file = os.path.join(gcn_cache_dir, cache_name)
                if os.path.exists(cache_file):
                    cache_found = True
                    actual_cache_name = cache_name
                    break
            if cache_found:
                print(f"     {sample_name}: {os.path.join(gcn_cache_dir, actual_cache_name)} âœ…")
            else:
                print(f"     {sample_name}: {os.path.join(gcn_cache_dir, f'{sample_name}.pkl')} âŒ")
        for sample_name in common_names:
            possible_cache_names = [
                f"{sample_name}.pkl",  
                f"{sample_name}.cif.pkl",  
                f"{sample_name}_relaxed.pkl",  
                f"{sample_name}_relaxed_interp_2.pkl",  
                f"{sample_name}_relaxed_interp_3.pkl"   
            ]
            cache_found = False
            actual_cache_name = None
            for cache_name in possible_cache_names:
                cache_file = os.path.join(gcn_cache_dir, cache_name)
                if os.path.exists(cache_file):
                    samples_with_gcn.append(sample_name)
                    cache_found = True
                    actual_cache_name = cache_name
                    break
            if not cache_found:
                samples_without_gcn.append(sample_name)
        print(f"âœ… æœ‰GCNç¼“å­˜çš„æ ·æœ¬: {len(samples_with_gcn)} ä¸ª")
        print(f"âš ï¸  æ— GCNç¼“å­˜çš„æ ·æœ¬: {len(samples_without_gcn)} ä¸ª")
        cache_coverage = len(samples_with_gcn) / len(common_names) * 100
        print(f"ğŸ“Š GCNç¼“å­˜è¦†ç›–ç‡: {cache_coverage:.2f}%")
        self.samples_without_gcn = set(samples_without_gcn)
        final_samples = sorted(list(common_names))
        print(f"ğŸ¯ æœ€ç»ˆå¤„ç†æ ·æœ¬æ•°: {len(final_samples)} ä¸ª")
        if samples_with_gcn:
            cache_format_stats = {}
            for sample_name in samples_with_gcn:
                possible_cache_names = [
                    f"{sample_name}.pkl",
                    f"{sample_name}.cif.pkl",
                    f"{sample_name}_relaxed.pkl",
                    f"{sample_name}_relaxed_interp_2.pkl",
                    f"{sample_name}_relaxed_interp_3.pkl"
                ]
                for cache_name in possible_cache_names:
                    cache_file = os.path.join(gcn_cache_dir, cache_name)
                    if os.path.exists(cache_file):
                        if cache_name.endswith('.cif.pkl'):
                            format_type = '.cif.pkl'
                        elif cache_name.endswith('_relaxed_interp_3.pkl'):
                            format_type = '_relaxed_interp_3.pkl'
                        elif cache_name.endswith('_relaxed_interp_2.pkl'):
                            format_type = '_relaxed_interp_2.pkl'
                        elif cache_name.endswith('_relaxed.pkl'):
                            format_type = '_relaxed.pkl'
                        else:
                            format_type = '.pkl'
                        cache_format_stats[format_type] = cache_format_stats.get(format_type, 0) + 1
                        break
            print(f"ğŸ“ ç¼“å­˜æ–‡ä»¶æ ¼å¼ç»Ÿè®¡:")
            for format_type, count in sorted(cache_format_stats.items()):
                print(f"     {format_type}: {count} ä¸ª")
        return final_samples
    def _get_topo_sum(self, sample_name):
        feat = self.topo_features_cache.get(sample_name)
        if feat is None:
            return 0.0
        if isinstance(feat, torch.Tensor):
            return feat.abs().sum().item()
        elif isinstance(feat, np.ndarray):
            return np.abs(feat).sum()
        else:
            return 0.0
    def _get_feat_sum(self, feat):
        if feat is None:
            return 0.0
        if isinstance(feat, torch.Tensor):
            return feat.abs().sum().item()
        elif isinstance(feat, np.ndarray):
            return np.abs(feat).sum()
        else:
            return 0.0
    def _ensure_tensor(self, feat):
        if feat is None:
            return torch.zeros(18, dtype=torch.float32)  
        if isinstance(feat, torch.Tensor):
            return feat
        elif isinstance(feat, np.ndarray):
            return torch.from_numpy(feat).float()
        else:
            try:
                return torch.tensor(feat, dtype=torch.float32)
            except:
                return torch.zeros(18, dtype=torch.float32)
    def __len__(self):
        return len(self.sample_identifiers)
    def __getitem__(self, idx):
        sample_name = self.sample_identifiers[idx]
        print(f"\nğŸ” å¤„ç†æ ·æœ¬ {idx + 1}/{len(self.sample_identifiers)}: {sample_name}")
        print("    ğŸ“¥ åŠ è½½VAEæ•°æ®...")
        vae_data = self._load_vae_data(sample_name)
        print("    ğŸ“¥ åŠ è½½æè¿°ç¬¦...")
        descriptors = self._load_descriptors(sample_name)
        if sample_name in self.samples_without_gcn:
            print("    âš ï¸  è·³è¿‡GCNæ•°æ®åŠ è½½ï¼ˆæ— ç¼“å­˜ï¼‰")
            import dgl
            empty_graph = dgl.heterograph({
                ("l", "l2n", "n"): (torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)),
                ("n", "n2l", "l"): (torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)),
            })
            empty_graph.nodes["l"].data["feat"] = torch.zeros(0, 300, dtype=torch.float32)
            empty_graph.nodes["n"].data["feat"] = torch.zeros(0, 4, dtype=torch.float32)
            graph_cc, graph_noncc = empty_graph, empty_graph
        else:
            print("    ğŸ“¥ åŠ è½½GCNå›¾æ•°æ®...")
            graph_cc, graph_noncc = self._load_gcn_data(sample_name)
        print("    ğŸ“¥ åŠ è½½LZHNNç‰¹å¾ï¼ˆä»ç¼“å­˜ï¼‰...")
        topo_features = self.topo_features_cache.get(sample_name, torch.zeros(18, dtype=torch.float32))
        print("    ğŸ“¥ åŠ è½½ç»“æ„ç‰¹å¾...")
        struct_features = self._load_struct_features(sample_name)
        print("    ğŸ“¥ ç”ŸæˆCCæ©ç ...")
        cc_mask = self._generate_cc_mask(sample_name)
        print(f"    âœ… æ ·æœ¬ {sample_name} æ•°æ®åŠ è½½å®Œæˆ")
        print(f"      æ‹“æ‰‘ç‰¹å¾: {topo_features.shape}, éé›¶å€¼: {self._get_feat_sum(topo_features):.2f}")
        print(f"      ç»“æ„ç‰¹å¾: {struct_features.shape}, éé›¶å€¼: {self._get_feat_sum(struct_features):.2f}")
        return {
            'vae_data': vae_data.unsqueeze(0),  
            'descriptors': descriptors.unsqueeze(0),  
            'graph_cc': graph_cc,
            'graph_noncc': graph_noncc,
            'lzhnn_data': {
                'topo': self._ensure_tensor(topo_features).unsqueeze(0),  
                'struct': self._ensure_tensor(struct_features).unsqueeze(0)  
            },
            'cc_mask': cc_mask.unsqueeze(0),  
            'identifiers': [sample_name]
        }
    def _load_vae_data(self, sample_name):
        h5_files = list(Path(self.data_config['vae_h5_dir']).glob(f"{sample_name}*.h5"))
        if not h5_files:
            print(f"âš ï¸  æœªæ‰¾åˆ°æ ·æœ¬ {sample_name} çš„H5æ–‡ä»¶")
            return torch.zeros(9, 2, 64, 64, dtype=torch.float32)
        h5_file = h5_files[0]
        print(f"    ğŸ“ æ‰¾åˆ°H5æ–‡ä»¶: {h5_file.name}")
        try:
            with h5py.File(h5_file, 'r') as f:
                planes = []
                for i in range(9):
                    if f'plane_{i}' in f:
                        plane_data = f[f'plane_{i}'][:]
                        if plane_data.ndim == 3 and plane_data.shape == (2, 64, 64):
                            planes.append(plane_data)
                        else:
                            print(f"    âš ï¸  å¹³é¢ {i} æ•°æ®æ ¼å¼ä¸æ­£ç¡®: {plane_data.shape}")
                            planes.append(np.zeros((2, 64, 64), dtype=np.float32))
                    else:
                        print(f"    âš ï¸  å¹³é¢ {i} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é›¶æ•°æ®")
                        planes.append(np.zeros((2, 64, 64), dtype=np.float32))
                planes = np.stack(planes, axis=0)
                print(f"    ğŸ“Š VAEæ•°æ®å½¢çŠ¶: {planes.shape}")
                return torch.from_numpy(planes).float()
        except Exception as e:
            print(f"âš ï¸  åŠ è½½H5æ–‡ä»¶ {h5_file} å¤±è´¥: {e}")
            return torch.zeros(9, 2, 64, 64, dtype=torch.float32)
    def _load_descriptors(self, sample_name):
        try:
            descriptor_row = self.descriptor_df[self.descriptor_df['name'] == sample_name]
            if not descriptor_row.empty:
                if self.descriptor_cols is not None:
                    available_cols = []
                    missing_cols = []
                    for train_col in self.descriptor_cols:
                        if train_col in self.descriptor_df.columns:
                            available_cols.append(train_col)
                        else:
                            missing_cols.append(train_col)
                    if missing_cols:
                        print(f"    âš ï¸  ç¼ºå¤±åˆ—: {missing_cols}")
                        print(f"    ğŸ“Š å¯ç”¨åˆ—: {available_cols}")
                    if available_cols:
                        descriptor_values = descriptor_row[available_cols].values[0]
                        print(f"    ğŸ“Š æå–åˆ° {len(available_cols)} ä¸ªç‰¹å¾")
                    else:
                        print(f"    âŒ æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾åˆ—")
                        return torch.zeros(len(self.descriptor_cols), dtype=torch.float32)
                    if len(available_cols) < len(self.descriptor_cols):
                        missing_count = len(self.descriptor_cols) - len(available_cols)
                        if hasattr(self, 'descriptor_scaler') and self.descriptor_scaler is not None:
                            mean_values = self.descriptor_scaler.mean_[len(available_cols):]
                            descriptor_values = np.concatenate([descriptor_values, mean_values])
                            print(f"    ğŸ“Š ç”¨è®­ç»ƒå‡å€¼å¡«å…… {missing_count} ä¸ªç¼ºå¤±ç‰¹å¾")
                        else:
                            padding = np.zeros(missing_count)
                            descriptor_values = np.concatenate([descriptor_values, padding])
                            print(f"    ğŸ“Š ç”¨é›¶å¡«å…… {missing_count} ä¸ªç¼ºå¤±ç‰¹å¾")
                    if len(descriptor_values) != len(self.descriptor_cols):
                        print(f"    âš ï¸  ç‰¹å¾æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(self.descriptor_cols)}, å®é™… {len(descriptor_values)}")
                        if len(descriptor_values) < len(self.descriptor_cols):
                            missing_count = len(self.descriptor_cols) - len(descriptor_values)
                            if hasattr(self, 'descriptor_scaler') and self.descriptor_scaler is not None:
                                mean_values = self.descriptor_scaler.mean_[len(descriptor_values):]
                                descriptor_values = np.concatenate([descriptor_values, mean_values])
                            else:
                                padding = np.zeros(missing_count)
                                descriptor_values = np.concatenate([descriptor_values, padding])
                        else:
                            descriptor_values = descriptor_values[:len(self.descriptor_cols)]
                else:
                    print(f"    âš ï¸  æè¿°ç¬¦æ ‡å‡†åŒ–å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤åˆ—")
                    descriptor_cols = ['%C', '%F', '%H', '%N', '%O', '%S', '%Si', 'PLD(?)', 'LCD(?)', 'surfacearea[m^2/g]', 'Porosity', 'Density(gr/cm3)']
                    available_cols = [col for col in descriptor_cols if col in self.descriptor_df.columns]
                    descriptor_values = descriptor_row[available_cols].values[0]
                descriptor_values = np.nan_to_num(descriptor_values, nan=0.0)
                if self.descriptor_scaler is not None:
                    if descriptor_values.ndim == 1:
                        descriptor_values = descriptor_values.reshape(1, -1)
                    descriptor_values = self.descriptor_scaler.transform(descriptor_values)[0]
                    print(f"    ğŸ“Š æè¿°ç¬¦å·²æ ‡å‡†åŒ–ï¼Œç‰¹å¾æ•°é‡: {len(descriptor_values)}")
                    if hasattr(self, 'train_descriptor_stats'):
                        expected_mean = self.train_descriptor_stats['mean']
                        expected_scale = self.train_descriptor_stats['scale']
                        if np.any(np.abs(descriptor_values) > 5):
                            print(f"    âš ï¸  è­¦å‘Š: æ ‡å‡†åŒ–åçš„ç‰¹å¾å€¼è¶…å‡ºé¢„æœŸèŒƒå›´")
                            print(f"       ç‰¹å¾å€¼èŒƒå›´: {descriptor_values.min():.4f} - {descriptor_values.max():.4f}")
                else:
                    print(f"    âš ï¸  æè¿°ç¬¦æœªæ ‡å‡†åŒ–")
                return torch.tensor(descriptor_values, dtype=torch.float32)
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ ·æœ¬ {sample_name} çš„æè¿°ç¬¦")
                if self.descriptor_cols is not None:
                    return torch.zeros(len(self.descriptor_cols), dtype=torch.float32)
                else:
                    return torch.zeros(12, dtype=torch.float32)  
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æè¿°ç¬¦å¤±è´¥: {e}")
            if self.descriptor_cols is not None:
                return torch.zeros(len(self.descriptor_cols), dtype=torch.float32)
            else:
                return torch.zeros(12, dtype=torch.float32)  
    def _load_gcn_data(self, sample_name):
        try:
            possible_cache_names = [
                f"{sample_name}.pkl",  
                f"{sample_name}.cif.pkl",  
                f"{sample_name}_relaxed.pkl",  
                f"{sample_name}_relaxed_interp_2.pkl",  
                f"{sample_name}_relaxed_interp_3.pkl"   
            ]
            cache_file = None
            for cache_name in possible_cache_names:
                temp_cache_file = Path(self.data_config['gcn_cache_dir']) / cache_name
                if temp_cache_file.exists():
                    cache_file = temp_cache_file
                    break
            if cache_file is not None:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)
                    print(f"    ğŸ“ ä»ç¼“å­˜åŠ è½½GCNæ•°æ®: {cache_file.name}")
                    import dgl
                    empty_graph = dgl.heterograph({
                        ("l", "l2n", "n"): (torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)),
                        ("n", "n2l", "l"): (torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)),
                    })
                    empty_graph.nodes["l"].data["feat"] = torch.zeros(0, 300, dtype=torch.float32)
                    empty_graph.nodes["n"].data["feat"] = torch.zeros(0, 4, dtype=torch.float32)
                    return cached_data, empty_graph
            else:
                print(f"    âš ï¸  æ— GCNç¼“å­˜ï¼Œä½¿ç”¨çœŸå®featurizerä»CIFç”Ÿæˆ...")
                generated_graph = self._generate_real_graph_from_cif(sample_name)
                if generated_graph is not None:
                    print(f"    âœ… æˆåŠŸä»CIFç”ŸæˆçœŸå®å›¾æ•°æ®")
                    return generated_graph, generated_graph
                else:
                    print(f"    âš ï¸  CIFç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨éšæœºå›¾æ•°æ®")
                    return self._generate_random_graph(), self._generate_random_graph()
        except Exception as e:
            print(f"âš ï¸  åŠ è½½GCNæ•°æ®å¤±è´¥: {e}")
            return self._generate_random_graph(), self._generate_random_graph()
    def _generate_real_graph_from_cif(self, sample_name):
        try:
            cif_dir = self.data_config['cif_dir']
            print(f"      ğŸ“ æœç´¢CIFç›®å½•: {cif_dir}")
            print(f"      ğŸ” æœç´¢æ¨¡å¼: {sample_name}*.cif")
            if not os.path.exists(cif_dir):
                print(f"      âŒ CIFç›®å½•ä¸å­˜åœ¨: {cif_dir}")
                return None
            cif_files = list(Path(cif_dir).glob(f"{sample_name}*.cif"))
            print(f"      ğŸ“Š æ‰¾åˆ° {len(cif_files)} ä¸ªåŒ¹é…çš„CIFæ–‡ä»¶")
            if not cif_files:
                print(f"      âš ï¸  æœªæ‰¾åˆ°CIFæ–‡ä»¶: {sample_name}")
                return None
            cif_file = cif_files[0]
            print(f"      ğŸ“ æ‰¾åˆ°CIFæ–‡ä»¶: {cif_file.name}")
            try:
                from GCN.featurizer import get_2cg_inputs_cof
                graph_data = get_2cg_inputs_cof(str(cif_file), "GCN/linkers.csv")
                if graph_data is not None:
                    print(f"      âœ… ä½¿ç”¨çœŸå®featurizeræˆåŠŸç”Ÿæˆå›¾æ•°æ®")
                    print(f"        èŠ‚ç‚¹æ•°: {graph_data.num_nodes('l')} linkers, {graph_data.num_nodes('n')} nodes")
                    print(f"        è¾¹æ•°: {graph_data.num_edges('l2n')}")
                    return graph_data
                else:
                    print(f"      âš ï¸  featurizerè¿”å›ç©ºå›¾")
                    return None
            except Exception as featurizer_error:
                print(f"      âš ï¸  çœŸå®featurizerå¤±è´¥: {featurizer_error}")
                return self._parse_cif_manually(cif_file)
        except Exception as e:
            print(f"      âš ï¸  ä»CIFç”ŸæˆçœŸå®å›¾æ•°æ®å¤±è´¥: {e}")
            return None
    def _parse_cif_manually(self, cif_file):
        try:
            import dgl
            import numpy as np
            with open(cif_file, 'r') as f:
                cif_content = f.read()
            lines = cif_content.split('\n')
            atoms = []
            for line in lines:
                if line.strip().startswith('_atom_site_'):
                    continue
                elif line.strip() and not line.startswith('#'):
                    parts = line.split()
                    if len(parts) >= 4:
                        try:
                            atom_type = parts[0]
                            x = float(parts[1])
                            y = float(parts[2])
                            z = float(parts[3])
                            atoms.append((atom_type, x, y, z))
                        except:
                            continue
            if len(atoms) == 0:
                print(f"        âš ï¸  æ— æ³•è§£æåŸå­åæ ‡")
                return None
            num_atoms = len(atoms)
            node_features = []
            for atom_type, x, y, z in atoms:
                if atom_type.startswith('C'):
                    atom_feat = [1.0, 0.0, 0.0, 0.0]  
                elif atom_type.startswith('N'):
                    atom_feat = [0.0, 1.0, 0.0, 0.0]  
                elif atom_type.startswith('O'):
                    atom_feat = [0.0, 0.0, 1.0, 0.0]  
                else:
                    atom_feat = [0.0, 0.0, 0.0, 1.0]  
                atom_feat.extend([x/10, y/10, z/10])  
                node_features.append(atom_feat)
            while len(node_features[0]) < 4:
                for i in range(len(node_features)):
                    node_features[i].append(0.0)
            node_features = [feat[:4] for feat in node_features]
            src_nodes = []
            dst_nodes = []
            for i in range(num_atoms):
                for j in range(num_atoms):
                    if i != j:
                        src_nodes.append(i)
                        dst_nodes.append(j)
            graph = dgl.heterograph({
                ("l", "l2n", "n"): (torch.tensor(src_nodes, dtype=torch.long), torch.tensor(dst_nodes, dtype=torch.long)),
                ("n", "n2l", "l"): (torch.tensor(dst_nodes, dtype=torch.long), torch.tensor(src_nodes, dtype=torch.long)),
            })
            graph.nodes["l"].data["feat"] = torch.tensor(node_features, dtype=torch.float32)
            graph.nodes["n"].data["feat"] = torch.tensor(node_features, dtype=torch.float32)
            print(f"        âœ… æ‰‹åŠ¨è§£æç”Ÿæˆå›¾: {num_atoms} ä¸ªåŸå­, {len(src_nodes)} æ¡è¾¹")
            return graph
        except Exception as e:
            print(f"        âš ï¸  æ‰‹åŠ¨è§£æCIFå¤±è´¥: {e}")
            return None
    def _generate_random_graph(self):
        try:
            import dgl
            import numpy as np
            num_nodes = np.random.randint(10, 50)
            node_features = torch.randn(num_nodes, 4, dtype=torch.float32) * 0.1
            num_edges = np.random.randint(num_nodes, num_nodes * 3)
            src_nodes = np.random.randint(0, num_nodes, num_edges)
            dst_nodes = np.random.randint(0, num_nodes, num_edges)
            graph = dgl.heterograph({
                ("l", "l2n", "n"): (torch.tensor(src_nodes, dtype=torch.long), torch.tensor(dst_nodes, dtype=torch.long)),
                ("n", "n2l", "l"): (torch.tensor(dst_nodes, dtype=torch.long), torch.tensor(src_nodes, dtype=torch.long)),
            })
            graph.nodes["l"].data["feat"] = node_features
            graph.nodes["n"].data["feat"] = node_features
            return graph
        except Exception as e:
            print(f"        âš ï¸  ç”Ÿæˆéšæœºå›¾å¤±è´¥: {e}")
            import dgl
            empty_graph = dgl.heterograph({
                ("l", "l2n", "n"): (torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)),
                ("n", "n2l", "l"): (torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)),
            })
            empty_graph.nodes["l"].data["feat"] = torch.zeros(0, 4, dtype=torch.float32)
            empty_graph.nodes["n"].data["feat"] = torch.zeros(0, 4, dtype=torch.float32)
            return empty_graph
    def _load_struct_features(self, sample_name):
        try:
            struct_row = self.struct_features_df[self.struct_features_df['name'] == sample_name]
            if not struct_row.empty:
                possible_struct_cols = [
                    ['PLD', 'LCD', 'surface_area', 'porosity', 'density'],
                    ['PLD(A)', 'LCD(A)', 'surfacearea[m^2/g]', 'Porosity', 'Density(gr/cm3)'],
                    ['pld', 'lcd', 'surface_area', 'porosity', 'density']
                ]
                struct_values = None
                used_cols = None
                for cols in possible_struct_cols:
                    available_cols = [col for col in cols if col in self.struct_features_df.columns]
                    if len(available_cols) >= 3:  
                        try:
                            struct_values = struct_row[available_cols].values[0]
                            used_cols = available_cols
                            break
                        except:
                            continue
                if struct_values is not None:
                    struct_values = np.nan_to_num(struct_values, nan=0.0)
                    if len(struct_values) < 5:
                        padding = np.zeros(5 - len(struct_values))
                        struct_values = np.concatenate([struct_values, padding])
                        print(f"    ğŸ“Š ç”¨é›¶å¡«å……åˆ°5ç»´ç‰¹å¾")
                    struct_tensor = torch.from_numpy(struct_values).float()
                    print(f"    ğŸ“Š ç»“æ„ç‰¹å¾: ä½¿ç”¨åˆ— {used_cols}")
                    return struct_tensor
                else:
                    print(f"âš ï¸  æœªæ‰¾åˆ°æ ·æœ¬ {sample_name} çš„æœ‰æ•ˆç»“æ„ç‰¹å¾åˆ—")
                    return torch.zeros(5, dtype=torch.float32)
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°æ ·æœ¬ {sample_name} çš„ç»“æ„ç‰¹å¾")
                return torch.zeros(5, dtype=torch.float32)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç»“æ„ç‰¹å¾å¤±è´¥: {e}")
            return torch.zeros(5, dtype=torch.float32)
    def _is_cc_structure(self, sample_name):
        return 'CC' in sample_name
    def _generate_cc_mask(self, sample_name):
        is_cc = self._is_cc_structure(sample_name)
        return torch.tensor([[is_cc]], dtype=torch.float32)  
def main():
    parser = argparse.ArgumentParser(description='ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®æ ¼å¼çš„èåˆæ¨¡å‹é¢„æµ‹æ–°æ•°æ®')
    parser.add_argument('--model_path', type=str, required=True, help='è®­ç»ƒå¥½çš„èåˆæ¨¡å‹è·¯å¾„')
    parser.add_argument('--vae_h5_dir', type=str, required=True, help='VAEçš„H5æ–‡ä»¶ç›®å½•')
    parser.add_argument('--descriptor_csv', type=str, required=True, help='æè¿°ç¬¦CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--cif_dir', type=str, required=True, help='CIFæ–‡ä»¶ç›®å½•')
    parser.add_argument('--struct_features_csv', type=str, required=True, help='ç»“æ„ç‰¹å¾CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--gcn_cache_dir', type=str, required=True, help='GCNç¼“å­˜ç›®å½•')
    parser.add_argument('--output_file', type=str, default='custom_predictions.csv', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--device', type=str, default='cuda', help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--use_parallel', action='store_true', default=True, help='å¯ç”¨å¹¶è¡Œç‰¹å¾æå–ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--no_parallel', dest='use_parallel', action='store_false', help='ç¦ç”¨å¹¶è¡Œç‰¹å¾æå–ï¼Œä½¿ç”¨ä¸²è¡Œå¤„ç†')
    parser.add_argument('--num_workers', type=int, default=None, help='å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°-1ï¼‰')
    args = parser.parse_args()
    print("ğŸ” éªŒè¯è¾“å…¥å‚æ•°...")
    required_files = [
        ('æ¨¡å‹æ–‡ä»¶', args.model_path),
        ('VAE H5ç›®å½•', args.vae_h5_dir),
        ('æè¿°ç¬¦CSV', args.descriptor_csv),
        ('CIFç›®å½•', args.cif_dir),
        ('ç»“æ„ç‰¹å¾CSV', args.struct_features_csv),
        ('GCNç¼“å­˜ç›®å½•', args.gcn_cache_dir)
    ]
    for name, path in required_files:
        if os.path.exists(path):
            if os.path.isfile(path):
                size = os.path.getsize(path)
                print(f"   âœ… {name}: {path} ({size} bytes)")
            else:
                file_count = len(list(Path(path).glob('*')))
                print(f"   âœ… {name}: {path} ({file_count} æ–‡ä»¶)")
        else:
            print(f"   âŒ {name}: {path} (ä¸å­˜åœ¨)")
            if name != 'GCNç¼“å­˜ç›®å½•':  
                print(f"     é”™è¯¯: {name}ä¸å­˜åœ¨ï¼Œæ— æ³•ç»§ç»­")
                sys.exit(1)
    try:
        print("\nğŸš€ åˆ›å»ºé¢„æµ‹å™¨...")
        if args.device == 'cuda' and torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸ“Š æ£€æµ‹åˆ°GPUå†…å­˜: {gpu_memory:.1f} GB")
            if gpu_memory < 8.0:
                print("âš ï¸  è­¦å‘Šï¼šGPUå†…å­˜è¾ƒå°ï¼Œå¯èƒ½å‡ºç°å†…å­˜ä¸è¶³é”™è¯¯")
                print("   å»ºè®®ï¼šä½¿ç”¨ --device cpu å‚æ•°è¿›è¡ŒCPUæ¨ç†")
                print("   æˆ–è€…ï¼šå‡å°‘batch_sizeæˆ–ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        predictor = CustomDataPredictor(args.model_path, args.device)
        data_config = {
            'vae_h5_dir': args.vae_h5_dir,
            'descriptor_csv': args.descriptor_csv,
            'cif_dir': args.cif_dir,
            'struct_features_csv': args.struct_features_csv,
            'gcn_cache_dir': args.gcn_cache_dir
        }
        print("\nğŸ“Š å‡†å¤‡æ–°æ•°æ®...")
        dataloader = predictor.prepare_new_data(
            data_config, 
            use_parallel=args.use_parallel, 
            num_workers=args.num_workers
        )
        if len(dataloader) == 0:
            print("âŒ é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„æ•°æ®æ ·æœ¬")
            sys.exit(1)
        print(f"\nğŸ§  å¼€å§‹é¢„æµ‹ {len(dataloader)} ä¸ªæ ·æœ¬...")
        results = predictor.predict(dataloader, args.output_file)
        print("\nğŸ‰ é¢„æµ‹å®Œæˆï¼")
        print(f"ğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡:")
        print(f"   æ ·æœ¬æ•°é‡: {len(results)}")
        if 'prediction_normalized' in results.columns:
            print(f"   æ ‡å‡†åŒ–é¢„æµ‹å€¼èŒƒå›´: {results['prediction_normalized'].min():.4f} - {results['prediction_normalized'].max():.4f}")
            print(f"   æ ‡å‡†åŒ–é¢„æµ‹å€¼å‡å€¼: {results['prediction_normalized'].mean():.4f}")
            print(f"   æ ‡å‡†åŒ–é¢„æµ‹å€¼æ ‡å‡†å·®: {results['prediction_normalized'].std():.4f}")
        if 'prediction_lg' in results.columns:
            print(f"   lgé¢„æµ‹å€¼èŒƒå›´: {results['prediction_lg'].min():.4f} - {results['prediction_lg'].max():.4f}")
            print(f"   lgé¢„æµ‹å€¼å‡å€¼: {results['prediction_lg'].mean():.4f}")
            print(f"   lgé¢„æµ‹å€¼æ ‡å‡†å·®: {results['prediction_lg'].std():.4f}")
        print(f"   ç»“æœåˆ—å: {list(results.columns)}")
        summary_file = args.output_file.replace('.csv', '_summary.txt')
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"é¢„æµ‹ç»“æœæ‘˜è¦\n")
            f.write(f"==========\n\n")
            f.write(f"æ ·æœ¬æ•°é‡: {len(results)}\n")
            f.write(f"æ¨¡å‹è·¯å¾„: {args.model_path}\n")
            f.write(f"è¾“å‡ºæ–‡ä»¶: {args.output_file}\n")
            f.write(f"é¢„æµ‹æ—¶é—´: {pd.Timestamp.now()}\n\n")
            if 'prediction_normalized' in results.columns:
                f.write(f"æ ‡å‡†åŒ–é¢„æµ‹å€¼ç»Ÿè®¡:\n")
                f.write(f"  èŒƒå›´: {results['prediction_normalized'].min():.6f} - {results['prediction_normalized'].max():.6f}\n")
                f.write(f"  å‡å€¼: {results['prediction_normalized'].mean():.6f}\n")
                f.write(f"  æ ‡å‡†å·®: {results['prediction_normalized'].std():.6f}\n\n")
            if 'prediction_lg' in results.columns:
                f.write(f"lgé¢„æµ‹å€¼ç»Ÿè®¡:\n")
                f.write(f"  èŒƒå›´: {results['prediction_lg'].min():.6f} - {results['prediction_lg'].max():.6f}\n")
                f.write(f"  å‡å€¼: {results['prediction_lg'].mean():.6f}\n")
                f.write(f"  æ ‡å‡†å·®: {results['prediction_lg'].std():.6f}\n")
        print(f"ğŸ“ ç»“æœæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    except Exception as e:
        print(f"âŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        if "CUDA error: out of memory" in str(e) or "out of memory" in str(e):
            print("\nğŸ”§ CUDAå†…å­˜ä¸è¶³è§£å†³æ–¹æ¡ˆ:")
            print("   1. ä½¿ç”¨CPUæ¨ç†: --device cpu")
            print("   2. å‡å°‘batch_sizeï¼ˆå½“å‰ä¸º1ï¼Œå·²æ˜¯æœ€å°å€¼ï¼‰")
            print("   3. æ¸…ç†GPUå†…å­˜: nvidia-smi æŸ¥çœ‹è¿›ç¨‹ï¼Œkillå ç”¨å†…å­˜çš„è¿›ç¨‹")
            print("   4. é‡å¯ç³»ç»Ÿé‡Šæ”¾GPUå†…å­˜")
            print("   5. ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æ¨¡å‹é‡åŒ–")
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
                cached_memory = torch.cuda.memory_reserved(0) / 1024**3
                print(f"\nğŸ“Š å½“å‰GPUå†…å­˜çŠ¶æ€:")
                print(f"   æ€»å†…å­˜: {gpu_memory:.1f} GB")
                print(f"   å·²åˆ†é…: {allocated_memory:.1f} GB")
                print(f"   å·²ç¼“å­˜: {cached_memory:.1f} GB")
                print(f"   å¯ç”¨å†…å­˜: {gpu_memory - cached_memory:.1f} GB")
        import traceback
        traceback.print_exc()
        sys.exit(1)
if __name__ == "__main__":
    main()